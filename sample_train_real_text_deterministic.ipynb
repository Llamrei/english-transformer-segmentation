{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:37.727067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 20:01:37.871185: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-24 20:01:37.906851: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-24 20:01:38.556734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-24 20:01:38.556813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-24 20:01:38.556819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "GPU_FROM = 2\n",
    "GPU_TO = 3  \n",
    "\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "logging.info(f\"Num GPUs visible:{len(visible_devices)}\")\n",
    "tf.config.set_visible_devices(visible_devices[GPU_FROM:GPU_TO],'GPU')\n",
    "\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "logging.info(f\"Num GPUs to be used: {len(visible_devices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/a/al3615/tf_210/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-24 20:01:39.865202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 20:01:40.567158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22296 MB memory:  -> device: 2, name: GeForce RTX 3090, pci bus id: 0000:86:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"AMDDebutsDual-CoreOpteronProcessorAMD'snewdual-coreOpteronchipisdesignedmainlyforcorporatecomputingapplications,includingdatabases,Webservices,andfinancialtransactions.\", shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:41.517033: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import html\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from segmentation.model import SpaceSegmentationTransformer\n",
    "from segmentation.model import LossWithVoids\n",
    "\n",
    "gap = 3\n",
    "len = 100\n",
    "\n",
    "punc_mapping = {ord(x):x for x in string.punctuation}\n",
    "entity_mapping = {f\" ?{k}\": v for k, v in html.entities.html5.items() if k.endswith(\";\") and v in string.punctuation}\n",
    "punc_mapping = {f' ?&?#?{k};': v for k, v in punc_mapping.items()}\n",
    "\n",
    "train, test = tfds.load('ag_news_subset', split=\"train\"), tfds.load('ag_news_subset', split=\"test\")\n",
    "\n",
    "def unescape(text):\n",
    "    for match, replace in punc_mapping.items():\n",
    "        text = tf.strings.regex_replace(text, match, replace)\n",
    "    for match, replace in entity_mapping.items():#\n",
    "        text = tf.strings.regex_replace(text, match, replace)\n",
    "    return text\n",
    "\n",
    "def join_title_desc(text_dict):\n",
    "    return text_dict['title'] + ' ' + text_dict['description']\n",
    "\n",
    "def remove_space(text):\n",
    "    return tf.strings.reduce_join(tf.strings.strip(tf.strings.split(text)), axis=-1)\n",
    "\n",
    "def space_after_letter(letter):\n",
    "    def replacer(x):\n",
    "        x = tf.strings.regex_replace(x, f\"({letter})\", r\"\\1 \") # add whitespace back in after every letter e\n",
    "        return x\n",
    "    return replacer\n",
    "\n",
    "train = train.map(join_title_desc).map(unescape).map(remove_space)\n",
    "test = test.map(join_title_desc).map(unescape).map(remove_space)\n",
    "\n",
    "for x in train.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(), dtype=string, numpy=b\"amddebutsdual-coreopteronprocessoramd'snewdual-coreopteronchipisdesignedmainlyforcorporatecomputinga\">, None), <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
      "       1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
      "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:42.005869: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_labels(text):\n",
    "    text = tf.strings.substr(text, 0, 100)\n",
    "    text = tf.strings.lower(text)\n",
    "    encoder_in = text\n",
    "    encoder_out = tf.strings.regex_replace(text, r\"[^e]\", \"1\") # 1 indicates character\n",
    "    encoder_out = tf.strings.regex_replace(encoder_out, r\"e\", \"2\") # 2 indicates space\n",
    "    encoder_out = tf.strings.bytes_split(encoder_out)\n",
    "    encoder_out = tf.strings.to_number(encoder_out)\n",
    "    encoder_out = tf.pad(encoder_out, [[0,100-tf.shape(encoder_out)[0]]])\n",
    "\n",
    "    return (encoder_in, None), encoder_out\n",
    "\n",
    "train = train.map(generate_labels)\n",
    "val = test.map(generate_labels)\n",
    "for x in train.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:45.144471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 's',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " 'h',\n",
       " 'c',\n",
       " 'u',\n",
       " 'p',\n",
       " 'm',\n",
       " 'f',\n",
       " 'g',\n",
       " 'b',\n",
       " 'y',\n",
       " 'w',\n",
       " 'k',\n",
       " 'v',\n",
       " '-',\n",
       " ',',\n",
       " '.',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '0',\n",
       " 'x',\n",
       " 'j',\n",
       " '2',\n",
       " '1',\n",
       " 'q',\n",
       " '&',\n",
       " ':',\n",
       " 'z',\n",
       " '4',\n",
       " '3',\n",
       " '\\\\',\n",
       " '5',\n",
       " '\"',\n",
       " '<',\n",
       " '>',\n",
       " '/',\n",
       " '7',\n",
       " '6',\n",
       " '$',\n",
       " '8',\n",
       " '9',\n",
       " ';',\n",
       " '?',\n",
       " '=',\n",
       " '#',\n",
       " '!',\n",
       " '*']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    output_sequence_length=100,\n",
    "    standardize=\"lower\",\n",
    "    split=\"character\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "tokenizer.adapt(train.take(1000).map(lambda x,y: x[0]))\n",
    "tokenizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(), dtype=string, numpy=b\"amddebutsdual-coreopteronprocessoramd'snewdual-coreopteronchipisdesignedmainlyforcorporatecomputinga\">, None), <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
      "       1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
      "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>)\n",
      "tf.Tensor(\n",
      "[ 3 16 11 11  2 19 14  4  5 11 14  3 10 24 13  6  9  2  6 15  4  2  9  6\n",
      "  8 15  9  6 13  2  5  5  6  9  3 16 11 27  5  8  2 21 11 14  3 10 24 13\n",
      "  6  9  2  6 15  4  2  9  6  8 13 12  7 15  7  5 11  2  5  7 18  8  2 11\n",
      " 16  3  7  8 10 20 17  6  9 13  6  9 15  6  9  3  4  2 13  6 16 15 14  4\n",
      "  7  8 18  3], shape=(100,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[False False False False  True False False False False False False False\n",
      " False False False False False  True False False False  True False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False  True False False False  True False False False False False\n",
      " False False False False False  True False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False], shape=(100,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:45.498410: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in train.take(1):\n",
    "    print(x)\n",
    "    tokens = tokenizer(x[0][0])\n",
    "    print(tokens)\n",
    "    space_tokens = tf.where(tokens == 2, tokens, 3) # Just happen to know e tokenizes to 2 - messy af\n",
    "    print(tf.cast(tf.where(x[1] == 2, x[1], 7), dtype=space_tokens.dtype) == space_tokens) # TODO: investigate what is going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpaceSegmentationTransformer(\n",
    "    num_layers=2,\n",
    "    d_model=512,\n",
    "    num_attention_heads=3,\n",
    "    seq_len=100,\n",
    "    dff=1028,\n",
    "    input_tokenizer=tokenizer,\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation.metrics import SparseAccuracyWithIgnore\n",
    "from segmentation.metrics import SparsePrecision\n",
    "from segmentation.metrics import SparseRecall\n",
    "from segmentation.metrics import SparseF1\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        SparseAccuracyWithIgnore(ignore_token=0.),\n",
    "        SparsePrecision(class_id=2, name=\"space_precision\"),\n",
    "        SparseRecall(class_id=2, name=\"space_recall\"),\n",
    "        SparseF1(class_id=2, name=\"space_f1\"),\n",
    "        SparsePrecision(class_id=1, name=\"char_precision\"),\n",
    "        SparseRecall(class_id=1, name=\"char_recall\"),\n",
    "        SparseF1(class_id=1, name=\"char_f1\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.Precision(class_id=2)\n",
    "# m.update_state([1., 2., 3., 4.],[4., 2., 2., 4.])\n",
    "# m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    7/15000 [..............................] - ETA: 5:41 - loss: 1.8805 - sparse_categorical_accuracy: 0.7785 - space_precision: 0.0568 - space_recall: 0.0242 - space_f1: 0.0339 - char_precision: 0.8824 - char_recall: 0.8733 - char_f1: 0.8779  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:01:50.999388: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 346s 23ms/step - loss: 1.7429 - sparse_categorical_accuracy: 0.8918 - space_precision: 0.0568 - space_recall: 1.1573e-05 - space_f1: 2.3140e-05 - char_precision: 0.8909 - char_recall: 0.9999 - char_f1: 0.9423 - val_loss: 1.7402 - val_sparse_categorical_accuracy: 0.8920 - val_space_precision: 0.0000e+00 - val_space_recall: 0.0000e+00 - val_space_f1: nan - val_char_precision: 0.8909 - val_char_recall: 1.0000 - val_char_f1: 0.9423\n",
      "Epoch 2/100\n",
      "15000/15000 [==============================] - 339s 23ms/step - loss: 1.7428 - sparse_categorical_accuracy: 0.8919 - space_precision: 0.0000e+00 - space_recall: 0.0000e+00 - space_f1: nan - char_precision: 0.8909 - char_recall: 1.0000 - char_f1: 0.9423 - val_loss: 1.7402 - val_sparse_categorical_accuracy: 0.8920 - val_space_precision: 0.0000e+00 - val_space_recall: 0.0000e+00 - val_space_f1: nan - val_char_precision: 0.8909 - val_char_recall: 1.0000 - val_char_f1: 0.9423\n",
      "Epoch 3/100\n",
      "15000/15000 [==============================] - 340s 23ms/step - loss: 1.7428 - sparse_categorical_accuracy: 0.8919 - space_precision: 0.0000e+00 - space_recall: 0.0000e+00 - space_f1: nan - char_precision: 0.8909 - char_recall: 1.0000 - char_f1: 0.9423 - val_loss: 1.7402 - val_sparse_categorical_accuracy: 0.8920 - val_space_precision: 0.0000e+00 - val_space_recall: 0.0000e+00 - val_space_f1: nan - val_char_precision: 0.8909 - val_char_recall: 1.0000 - val_char_f1: 0.9423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7600240e20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train.shuffle(100).batch(8)\n",
    "val_ds = val.batch(8)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_space_precision\",\n",
    "        patience=2,\n",
    "        mode=\"max\"\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950/950 [==============================] - 13s 13ms/step - loss: 1.7402 - sparse_categorical_accuracy: 0.8920 - space_precision: 0.0000e+00 - space_recall: 0.0000e+00 - space_f1: nan - char_precision: 0.8909 - char_recall: 1.0000 - char_f1: 0.9423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.740188479423523,\n",
       " 0.8920347094535828,\n",
       " 0.0,\n",
       " 0.0,\n",
       " nan,\n",
       " 0.89094078540802,\n",
       " 1.0,\n",
       " 0.9423254132270813]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 100, 3), dtype=float32, numpy=\n",
       " array([[[1.05425354e-16, 1.00000000e+00, 2.74945489e-14],\n",
       "         [1.08670727e-16, 1.00000000e+00, 2.77842290e-14],\n",
       "         [1.05154641e-16, 1.00000000e+00, 2.48668443e-14],\n",
       "         [1.02765776e-16, 1.00000000e+00, 2.47171329e-14],\n",
       "         [1.00100706e-16, 1.00000000e+00, 2.49156978e-14],\n",
       "         [1.01054895e-16, 1.00000000e+00, 2.43982504e-14],\n",
       "         [1.04804247e-16, 1.00000000e+00, 2.39520978e-14],\n",
       "         [9.52140935e-17, 1.00000000e+00, 2.66245461e-14],\n",
       "         [9.57075166e-17, 1.00000000e+00, 2.15318740e-14],\n",
       "         [9.82125239e-17, 1.00000000e+00, 2.65174117e-14],\n",
       "         [1.02147078e-16, 1.00000000e+00, 2.36837968e-14],\n",
       "         [6.09608421e-17, 1.00000000e+00, 7.09642712e-14],\n",
       "         [9.24537918e-17, 1.00000000e+00, 2.20068646e-14],\n",
       "         [9.35343279e-17, 1.00000000e+00, 2.49943753e-14],\n",
       "         [9.35714451e-17, 1.00000000e+00, 2.55101065e-14],\n",
       "         [9.28832970e-17, 1.00000000e+00, 2.59816616e-14],\n",
       "         [9.16115631e-17, 1.00000000e+00, 2.64759697e-14],\n",
       "         [9.01417293e-17, 1.00000000e+00, 2.66469027e-14],\n",
       "         [8.88421028e-17, 1.00000000e+00, 2.67245739e-14],\n",
       "         [8.78140960e-17, 1.00000000e+00, 2.65861856e-14],\n",
       "         [8.72737618e-17, 1.00000000e+00, 2.61234787e-14],\n",
       "         [8.72671046e-17, 1.00000000e+00, 2.55060679e-14],\n",
       "         [8.79914832e-17, 1.00000000e+00, 2.49930895e-14],\n",
       "         [8.89173697e-17, 1.00000000e+00, 2.45912028e-14],\n",
       "         [8.93127223e-17, 1.00000000e+00, 2.43837814e-14],\n",
       "         [8.89523164e-17, 1.00000000e+00, 2.44014624e-14],\n",
       "         [8.80022234e-17, 1.00000000e+00, 2.43048921e-14],\n",
       "         [8.71237377e-17, 1.00000000e+00, 2.39379846e-14],\n",
       "         [8.75134655e-17, 1.00000000e+00, 2.32484015e-14],\n",
       "         [8.90840763e-17, 1.00000000e+00, 2.25779529e-14],\n",
       "         [9.10423040e-17, 1.00000000e+00, 2.21240330e-14],\n",
       "         [9.24954155e-17, 1.00000000e+00, 2.20346253e-14],\n",
       "         [9.25398847e-17, 1.00000000e+00, 2.23186575e-14],\n",
       "         [9.12122996e-17, 1.00000000e+00, 2.27401529e-14],\n",
       "         [8.97399578e-17, 1.00000000e+00, 2.29517960e-14],\n",
       "         [8.88695586e-17, 1.00000000e+00, 2.28630557e-14],\n",
       "         [8.92109195e-17, 1.00000000e+00, 2.26433320e-14],\n",
       "         [9.02160366e-17, 1.00000000e+00, 2.23514614e-14],\n",
       "         [9.12516204e-17, 1.00000000e+00, 2.22727720e-14],\n",
       "         [9.13564540e-17, 1.00000000e+00, 2.24364594e-14],\n",
       "         [9.09066133e-17, 1.00000000e+00, 2.25809683e-14],\n",
       "         [9.07444661e-17, 1.00000000e+00, 2.25001428e-14],\n",
       "         [9.08168410e-17, 1.00000000e+00, 2.22197748e-14],\n",
       "         [9.11907201e-17, 1.00000000e+00, 2.19605726e-14],\n",
       "         [9.13345039e-17, 1.00000000e+00, 2.18298585e-14],\n",
       "         [9.14952681e-17, 1.00000000e+00, 2.18054724e-14],\n",
       "         [9.15196997e-17, 1.00000000e+00, 2.17738035e-14],\n",
       "         [9.14739732e-17, 1.00000000e+00, 2.17162764e-14],\n",
       "         [9.15486841e-17, 1.00000000e+00, 2.15594466e-14],\n",
       "         [9.23526243e-17, 1.00000000e+00, 2.14049359e-14],\n",
       "         [9.29712098e-17, 1.00000000e+00, 2.13376781e-14],\n",
       "         [9.33215572e-17, 1.00000000e+00, 2.13899994e-14],\n",
       "         [9.37572431e-17, 1.00000000e+00, 2.14746179e-14],\n",
       "         [9.44212442e-17, 1.00000000e+00, 2.14352512e-14],\n",
       "         [9.52220874e-17, 1.00000000e+00, 2.12186429e-14],\n",
       "         [9.59725850e-17, 1.00000000e+00, 2.08126024e-14],\n",
       "         [9.64968058e-17, 1.00000000e+00, 2.04412564e-14],\n",
       "         [9.66493246e-17, 1.00000000e+00, 2.02604911e-14],\n",
       "         [9.61646166e-17, 1.00000000e+00, 2.03254788e-14],\n",
       "         [9.59191359e-17, 1.00000000e+00, 2.03677797e-14],\n",
       "         [9.64968058e-17, 1.00000000e+00, 2.00796767e-14],\n",
       "         [9.74352256e-17, 1.00000000e+00, 1.95179108e-14],\n",
       "         [9.81117932e-17, 1.00000000e+00, 1.88008636e-14],\n",
       "         [9.80627844e-17, 1.00000000e+00, 1.83006178e-14],\n",
       "         [9.70453655e-17, 1.00000000e+00, 1.80929964e-14],\n",
       "         [9.63643775e-17, 1.00000000e+00, 1.82988356e-14],\n",
       "         [9.63144025e-17, 1.00000000e+00, 1.87299940e-14],\n",
       "         [9.76093306e-17, 1.00000000e+00, 1.92192369e-14],\n",
       "         [9.91907146e-17, 1.00000000e+00, 1.96457196e-14],\n",
       "         [1.00134323e-16, 1.00000000e+00, 1.98901158e-14],\n",
       "         [1.00050314e-16, 1.00000000e+00, 1.98956926e-14],\n",
       "         [9.93065663e-17, 1.00000000e+00, 1.96706919e-14],\n",
       "         [9.93508965e-17, 1.00000000e+00, 1.92160469e-14],\n",
       "         [1.00260835e-16, 1.00000000e+00, 1.85746329e-14],\n",
       "         [1.01397414e-16, 1.00000000e+00, 1.79875696e-14],\n",
       "         [1.02123699e-16, 1.00000000e+00, 1.76378416e-14],\n",
       "         [1.01230066e-16, 1.00000000e+00, 1.75721949e-14],\n",
       "         [9.92043400e-17, 1.00000000e+00, 1.78248970e-14],\n",
       "         [9.70816489e-17, 1.00000000e+00, 1.81932496e-14],\n",
       "         [9.61668136e-17, 1.00000000e+00, 1.84816541e-14],\n",
       "         [9.67607293e-17, 1.00000000e+00, 1.86481842e-14],\n",
       "         [9.79468865e-17, 1.00000000e+00, 1.88218886e-14],\n",
       "         [9.89035639e-17, 1.00000000e+00, 1.89105526e-14],\n",
       "         [9.89654568e-17, 1.00000000e+00, 1.88500914e-14],\n",
       "         [9.88221031e-17, 1.00000000e+00, 1.85913991e-14],\n",
       "         [9.93088427e-17, 1.00000000e+00, 1.82768263e-14],\n",
       "         [1.00736212e-16, 1.00000000e+00, 1.78990987e-14],\n",
       "         [1.02200481e-16, 1.00000000e+00, 1.76371335e-14],\n",
       "         [1.03324738e-16, 1.00000000e+00, 1.76327611e-14],\n",
       "         [1.03401243e-16, 1.00000000e+00, 1.78918617e-14],\n",
       "         [1.02849308e-16, 1.00000000e+00, 1.81705000e-14],\n",
       "         [1.02728149e-16, 1.00000000e+00, 1.82519168e-14],\n",
       "         [1.03392170e-16, 1.00000000e+00, 1.81426563e-14],\n",
       "         [1.03915292e-16, 1.00000000e+00, 1.79437747e-14],\n",
       "         [1.03950974e-16, 1.00000000e+00, 1.79071930e-14],\n",
       "         [1.02674085e-16, 1.00000000e+00, 1.80690288e-14],\n",
       "         [1.00764270e-16, 1.00000000e+00, 1.83054696e-14],\n",
       "         [9.93580963e-17, 1.00000000e+00, 1.83172738e-14],\n",
       "         [9.92149345e-17, 1.00000000e+00, 1.81225969e-14],\n",
       "         [1.00286464e-16, 1.00000000e+00, 1.77204883e-14]],\n",
       " \n",
       "        [[7.19904485e-17, 1.00000000e+00, 6.53491679e-14],\n",
       "         [7.19569576e-17, 1.00000000e+00, 6.78116417e-14],\n",
       "         [7.06578407e-17, 1.00000000e+00, 6.84294608e-14],\n",
       "         [6.91372644e-17, 1.00000000e+00, 6.75814249e-14],\n",
       "         [6.82749915e-17, 1.00000000e+00, 6.61650300e-14],\n",
       "         [6.82054884e-17, 1.00000000e+00, 6.45784627e-14],\n",
       "         [6.85348916e-17, 1.00000000e+00, 6.34974048e-14],\n",
       "         [6.86814548e-17, 1.00000000e+00, 6.28563499e-14],\n",
       "         [6.80773416e-17, 1.00000000e+00, 6.25868172e-14],\n",
       "         [6.70354514e-17, 1.00000000e+00, 6.24716072e-14],\n",
       "         [6.61168574e-17, 1.00000000e+00, 6.26122553e-14],\n",
       "         [6.57644454e-17, 1.00000000e+00, 6.30099949e-14],\n",
       "         [6.60097607e-17, 1.00000000e+00, 6.35154500e-14],\n",
       "         [6.65706354e-17, 1.00000000e+00, 6.46596898e-14],\n",
       "         [6.67809842e-17, 1.00000000e+00, 6.58416193e-14],\n",
       "         [6.64371947e-17, 1.00000000e+00, 6.69919645e-14],\n",
       "         [6.57619440e-17, 1.00000000e+00, 6.78516217e-14],\n",
       "         [6.48976792e-17, 1.00000000e+00, 6.85527752e-14],\n",
       "         [6.40201133e-17, 1.00000000e+00, 6.87830259e-14],\n",
       "         [6.34460963e-17, 1.00000000e+00, 6.85287195e-14],\n",
       "         [6.32457796e-17, 1.00000000e+00, 6.74248594e-14],\n",
       "         [8.81988276e-17, 1.00000000e+00, 2.57413906e-14],\n",
       "         [8.89492657e-17, 1.00000000e+00, 2.52395608e-14],\n",
       "         [8.96855491e-17, 1.00000000e+00, 2.48248552e-14],\n",
       "         [9.01661477e-17, 1.00000000e+00, 2.46639172e-14],\n",
       "         [8.98807770e-17, 1.00000000e+00, 2.46922064e-14],\n",
       "         [8.89553737e-17, 1.00000000e+00, 2.45946740e-14],\n",
       "         [8.82119500e-17, 1.00000000e+00, 2.42048254e-14],\n",
       "         [8.85629459e-17, 1.00000000e+00, 2.35428268e-14],\n",
       "         [8.99631046e-17, 1.00000000e+00, 2.28636673e-14],\n",
       "         [9.19690772e-17, 1.00000000e+00, 2.24357750e-14],\n",
       "         [9.32998520e-17, 1.00000000e+00, 2.23326250e-14],\n",
       "         [9.32265638e-17, 1.00000000e+00, 2.25963047e-14],\n",
       "         [9.20694638e-17, 1.00000000e+00, 2.30447036e-14],\n",
       "         [9.05729286e-17, 1.00000000e+00, 2.32451218e-14],\n",
       "         [8.96657034e-17, 1.00000000e+00, 2.31806998e-14],\n",
       "         [8.99044344e-17, 1.00000000e+00, 2.29249332e-14],\n",
       "         [9.10190371e-17, 1.00000000e+00, 2.26921024e-14],\n",
       "         [9.19764358e-17, 1.00000000e+00, 2.25902721e-14],\n",
       "         [9.20462829e-17, 1.00000000e+00, 2.26872540e-14],\n",
       "         [9.17437598e-17, 1.00000000e+00, 2.28515903e-14],\n",
       "         [9.14673425e-17, 1.00000000e+00, 2.28030858e-14],\n",
       "         [9.15190049e-17, 1.00000000e+00, 2.25274968e-14],\n",
       "         [9.17955678e-17, 1.00000000e+00, 2.22355466e-14],\n",
       "         [9.20445293e-17, 1.00000000e+00, 2.21133164e-14],\n",
       "         [9.22037318e-17, 1.00000000e+00, 2.20697484e-14],\n",
       "         [9.22839551e-17, 1.00000000e+00, 2.20617930e-14],\n",
       "         [9.21798097e-17, 1.00000000e+00, 2.20384098e-14],\n",
       "         [9.22209636e-17, 1.00000000e+00, 2.18823830e-14],\n",
       "         [9.29662467e-17, 1.00000000e+00, 2.16973961e-14],\n",
       "         [9.36053595e-17, 1.00000000e+00, 2.16762203e-14],\n",
       "         [9.39469917e-17, 1.00000000e+00, 2.17115144e-14],\n",
       "         [9.43787536e-17, 1.00000000e+00, 2.18099651e-14],\n",
       "         [9.50315645e-17, 1.00000000e+00, 2.17838561e-14],\n",
       "         [9.57495109e-17, 1.00000000e+00, 2.15076573e-14],\n",
       "         [9.66087796e-17, 1.00000000e+00, 2.11332179e-14],\n",
       "         [9.70827607e-17, 1.00000000e+00, 2.07507283e-14],\n",
       "         [9.71131347e-17, 1.00000000e+00, 2.05607541e-14],\n",
       "         [9.67249356e-17, 1.00000000e+00, 2.06276087e-14],\n",
       "         [9.66508003e-17, 1.00000000e+00, 2.06755457e-14],\n",
       "         [9.70475889e-17, 1.00000000e+00, 2.03803310e-14],\n",
       "         [9.80313648e-17, 1.00000000e+00, 1.98372253e-14],\n",
       "         [9.87900614e-17, 1.00000000e+00, 1.91637071e-14],\n",
       "         [9.86036943e-17, 1.00000000e+00, 1.86433832e-14],\n",
       "         [9.76551366e-17, 1.00000000e+00, 1.84370799e-14],\n",
       "         [9.69720905e-17, 1.00000000e+00, 1.86266425e-14],\n",
       "         [9.71112885e-17, 1.00000000e+00, 1.90865915e-14],\n",
       "         [9.80923379e-17, 1.00000000e+00, 1.95430931e-14],\n",
       "         [9.96659597e-17, 1.00000000e+00, 2.00120004e-14],\n",
       "         [1.00574561e-16, 1.00000000e+00, 2.02406383e-14],\n",
       "         [1.00535055e-16, 1.00000000e+00, 2.02162150e-14],\n",
       "         [9.97846502e-17, 1.00000000e+00, 1.99982260e-14],\n",
       "         [9.98048268e-17, 1.00000000e+00, 1.95574114e-14],\n",
       "         [1.00787339e-16, 1.00000000e+00, 1.89522233e-14],\n",
       "         [1.01955159e-16, 1.00000000e+00, 1.83752227e-14],\n",
       "         [1.02410439e-16, 1.00000000e+00, 1.79894551e-14],\n",
       "         [1.01538300e-16, 1.00000000e+00, 1.79009758e-14],\n",
       "         [9.96594944e-17, 1.00000000e+00, 1.81480553e-14],\n",
       "         [9.77464507e-17, 1.00000000e+00, 1.85010512e-14],\n",
       "         [9.68220267e-17, 1.00000000e+00, 1.87766384e-14],\n",
       "         [9.73505157e-17, 1.00000000e+00, 1.89637226e-14],\n",
       "         [9.85059414e-17, 1.00000000e+00, 1.91598700e-14],\n",
       "         [9.93781869e-17, 1.00000000e+00, 1.92236719e-14],\n",
       "         [9.95166569e-17, 1.00000000e+00, 1.91631599e-14],\n",
       "         [9.93615109e-17, 1.00000000e+00, 1.89242288e-14],\n",
       "         [9.98817678e-17, 1.00000000e+00, 1.86131475e-14],\n",
       "         [1.01280669e-16, 1.00000000e+00, 1.82349033e-14],\n",
       "         [1.02795581e-16, 1.00000000e+00, 1.79921673e-14],\n",
       "         [1.03762787e-16, 1.00000000e+00, 1.79824231e-14],\n",
       "         [1.03828114e-16, 1.00000000e+00, 1.82435650e-14],\n",
       "         [1.03360221e-16, 1.00000000e+00, 1.85111122e-14],\n",
       "         [1.03286906e-16, 1.00000000e+00, 1.86339269e-14],\n",
       "         [1.03676535e-16, 1.00000000e+00, 1.85384240e-14],\n",
       "         [1.04364200e-16, 1.00000000e+00, 1.83565863e-14],\n",
       "         [1.04310063e-16, 1.00000000e+00, 1.82999876e-14],\n",
       "         [1.03156179e-16, 1.00000000e+00, 1.84873648e-14],\n",
       "         [1.01247053e-16, 1.00000000e+00, 1.86932345e-14],\n",
       "         [9.98890073e-17, 1.00000000e+00, 1.86990468e-14],\n",
       "         [9.98417653e-17, 1.00000000e+00, 1.85033822e-14],\n",
       "         [1.00945476e-16, 1.00000000e+00, 1.81139910e-14]],\n",
       " \n",
       "        [[1.06333724e-16, 1.00000000e+00, 2.40662626e-14],\n",
       "         [1.02336628e-16, 1.00000000e+00, 2.54055166e-14],\n",
       "         [6.53194089e-17, 1.00000000e+00, 7.66993619e-14],\n",
       "         [9.67308383e-17, 1.00000000e+00, 2.71921157e-14],\n",
       "         [9.96594944e-17, 1.00000000e+00, 2.74038706e-14],\n",
       "         [9.87595418e-17, 1.00000000e+00, 2.46291296e-14],\n",
       "         [9.85694689e-17, 1.00000000e+00, 2.39428719e-14],\n",
       "         [9.45683169e-17, 1.00000000e+00, 2.65144267e-14],\n",
       "         [9.17801624e-17, 1.00000000e+00, 2.67355362e-14],\n",
       "         [9.36635798e-17, 1.00000000e+00, 2.13777225e-14],\n",
       "         [9.84019086e-17, 1.00000000e+00, 2.17561598e-14],\n",
       "         [9.67334191e-17, 1.00000000e+00, 2.55298695e-14],\n",
       "         [9.63996749e-17, 1.00000000e+00, 2.44230651e-14],\n",
       "         [9.43614754e-17, 1.00000000e+00, 2.57034181e-14],\n",
       "         [9.81806874e-17, 1.00000000e+00, 2.30857051e-14],\n",
       "         [8.73500345e-17, 1.00000000e+00, 3.02827933e-14],\n",
       "         [9.40337597e-17, 1.00000000e+00, 2.56298871e-14],\n",
       "         [9.34244982e-17, 1.00000000e+00, 2.90140091e-14],\n",
       "         [9.71861384e-17, 1.00000000e+00, 2.62697816e-14],\n",
       "         [9.06593525e-17, 1.00000000e+00, 2.89897805e-14],\n",
       "         [5.83744204e-17, 1.00000000e+00, 7.49126171e-14],\n",
       "         [8.96263826e-17, 1.00000000e+00, 2.87956880e-14],\n",
       "         [9.29127116e-17, 1.00000000e+00, 2.27613728e-14],\n",
       "         [8.61941654e-17, 1.00000000e+00, 2.61013169e-14],\n",
       "         [5.96073232e-17, 1.00000000e+00, 6.87476131e-14],\n",
       "         [8.77916563e-17, 1.00000000e+00, 2.17646268e-14],\n",
       "         [9.20919499e-17, 1.00000000e+00, 2.39134342e-14],\n",
       "         [8.74060313e-17, 1.00000000e+00, 2.44056975e-14],\n",
       "         [5.92941327e-17, 1.00000000e+00, 6.36758102e-14],\n",
       "         [9.73145036e-17, 1.00000000e+00, 2.21194336e-14],\n",
       "         [9.06064460e-17, 1.00000000e+00, 2.12909253e-14],\n",
       "         [9.70176053e-17, 1.00000000e+00, 2.33487444e-14],\n",
       "         [9.44288079e-17, 1.00000000e+00, 2.29566545e-14],\n",
       "         [9.31952699e-17, 1.00000000e+00, 2.56419167e-14],\n",
       "         [9.54297163e-17, 1.00000000e+00, 2.10673764e-14],\n",
       "         [9.62883165e-17, 1.00000000e+00, 2.24685772e-14],\n",
       "         [8.95354853e-17, 1.00000000e+00, 2.26758343e-14],\n",
       "         [9.05984985e-17, 1.00000000e+00, 2.24127645e-14],\n",
       "         [9.15713886e-17, 1.00000000e+00, 2.23373532e-14],\n",
       "         [9.16839315e-17, 1.00000000e+00, 2.24765512e-14],\n",
       "         [9.12947992e-17, 1.00000000e+00, 2.26086341e-14],\n",
       "         [9.10020302e-17, 1.00000000e+00, 2.25127195e-14],\n",
       "         [9.10252906e-17, 1.00000000e+00, 2.22346962e-14],\n",
       "         [9.13386861e-17, 1.00000000e+00, 2.19739794e-14],\n",
       "         [9.16349822e-17, 1.00000000e+00, 2.18490201e-14],\n",
       "         [9.19645178e-17, 1.00000000e+00, 2.18487270e-14],\n",
       "         [9.17934700e-17, 1.00000000e+00, 2.18037665e-14],\n",
       "         [9.16982715e-17, 1.00000000e+00, 2.17391938e-14],\n",
       "         [9.19669662e-17, 1.00000000e+00, 2.16225794e-14],\n",
       "         [9.25702522e-17, 1.00000000e+00, 2.14304265e-14],\n",
       "         [9.33261894e-17, 1.00000000e+00, 2.14023237e-14],\n",
       "         [9.36707333e-17, 1.00000000e+00, 2.14602455e-14],\n",
       "         [9.39781731e-17, 1.00000000e+00, 2.15271103e-14],\n",
       "         [9.46524048e-17, 1.00000000e+00, 2.14940811e-14],\n",
       "         [9.54566625e-17, 1.00000000e+00, 2.12546113e-14],\n",
       "         [9.62604042e-17, 1.00000000e+00, 2.08625214e-14],\n",
       "         [9.67393219e-17, 1.00000000e+00, 2.04782124e-14],\n",
       "         [9.68379086e-17, 1.00000000e+00, 2.03015349e-14],\n",
       "         [9.64695684e-17, 1.00000000e+00, 2.03741510e-14],\n",
       "         [9.62523243e-17, 1.00000000e+00, 2.04118813e-14],\n",
       "         [9.67644219e-17, 1.00000000e+00, 2.01334819e-14],\n",
       "         [9.76599805e-17, 1.00000000e+00, 1.95474926e-14],\n",
       "         [9.84390722e-17, 1.00000000e+00, 1.88574623e-14],\n",
       "         [9.83673854e-17, 1.00000000e+00, 1.83527001e-14],\n",
       "         [9.73965731e-17, 1.00000000e+00, 1.81691481e-14],\n",
       "         [9.66187256e-17, 1.00000000e+00, 1.83440570e-14],\n",
       "         [9.67393219e-17, 1.00000000e+00, 1.88089697e-14],\n",
       "         [9.78624743e-17, 1.00000000e+00, 1.92816175e-14],\n",
       "         [9.94779448e-17, 1.00000000e+00, 1.97235145e-14],\n",
       "         [1.00398610e-16, 1.00000000e+00, 1.99408649e-14],\n",
       "         [1.00377553e-16, 1.00000000e+00, 1.99416645e-14],\n",
       "         [9.95534830e-17, 1.00000000e+00, 1.97369095e-14],\n",
       "         [9.96199618e-17, 1.00000000e+00, 1.92717648e-14],\n",
       "         [1.00563821e-16, 1.00000000e+00, 1.86633444e-14],\n",
       "         [1.01728664e-16, 1.00000000e+00, 1.80674093e-14],\n",
       "         [1.02191898e-16, 1.00000000e+00, 1.76899951e-14],\n",
       "         [1.01329360e-16, 1.00000000e+00, 1.76255664e-14],\n",
       "         [9.93816015e-17, 1.00000000e+00, 1.78723545e-14],\n",
       "         [9.73575699e-17, 1.00000000e+00, 1.82362941e-14],\n",
       "         [9.64158479e-17, 1.00000000e+00, 1.85259827e-14],\n",
       "         [9.72421352e-17, 1.00000000e+00, 1.87221369e-14],\n",
       "         [9.81930422e-17, 1.00000000e+00, 1.88629663e-14],\n",
       "         [9.90545872e-17, 1.00000000e+00, 1.89502717e-14],\n",
       "         [9.92471085e-17, 1.00000000e+00, 1.88959515e-14],\n",
       "         [9.90477845e-17, 1.00000000e+00, 1.86378368e-14],\n",
       "         [9.96005859e-17, 1.00000000e+00, 1.83279685e-14],\n",
       "         [1.00949711e-16, 1.00000000e+00, 1.79503832e-14],\n",
       "         [1.02531221e-16, 1.00000000e+00, 1.77113302e-14],\n",
       "         [1.03479368e-16, 1.00000000e+00, 1.76792005e-14],\n",
       "         [1.03597066e-16, 1.00000000e+00, 1.79525398e-14],\n",
       "         [1.03169566e-16, 1.00000000e+00, 1.82483677e-14],\n",
       "         [1.03086947e-16, 1.00000000e+00, 1.83543451e-14],\n",
       "         [1.03521217e-16, 1.00000000e+00, 1.82285404e-14],\n",
       "         [1.04170891e-16, 1.00000000e+00, 1.80106039e-14],\n",
       "         [1.04189969e-16, 1.00000000e+00, 1.79814964e-14],\n",
       "         [1.02869326e-16, 1.00000000e+00, 1.81470168e-14],\n",
       "         [1.01018287e-16, 1.00000000e+00, 1.83739962e-14],\n",
       "         [9.96469544e-17, 1.00000000e+00, 1.83993852e-14],\n",
       "         [9.95645010e-17, 1.00000000e+00, 1.81955400e-14],\n",
       "         [1.00661303e-16, 1.00000000e+00, 1.78087203e-14]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 100), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model([(\"dunningkruger\", \"eeeeeeeeeeeeeeeeeeeee\", \"thequickbrownfoxjumpedoverthelazydog\"),(None,None, None)])\n",
    "preds, tf.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100, 512), dtype=float32, numpy=\n",
       "array([[[ 0.08132914,  0.08854049, -0.14747757, ..., -0.0485481 ,\n",
       "          1.4191325 ,  1.0945144 ],\n",
       "        [ 1.9637349 ,  0.90921146, -0.16049284, ...,  2.1188207 ,\n",
       "          0.3337893 ,  0.89834803],\n",
       "        [ 2.0315614 ,  1.02377   , -0.00431025, ...,  2.1188207 ,\n",
       "          0.3337893 ,  0.89834803],\n",
       "        ...,\n",
       "        [ 1.5018716 , -0.5380123 , -0.21734512, ...,  2.1187623 ,\n",
       "          0.333735  ,  0.8982975 ],\n",
       "        [ 0.548882  ,  0.37240577, -1.0522068 , ...,  2.118761  ,\n",
       "          0.33373386,  0.8982964 ],\n",
       "        [ 0.12305707,  1.0375065 , -1.8147955 , ...,  2.1187599 ,\n",
       "          0.33373272,  0.8982954 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.pos_embedding(tokenizer([\"e\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100, 512), dtype=float32, numpy=\n",
       "array([[[-0.26644796,  0.7871986 , -0.2654642 , ...,  2.1398592 ,\n",
       "          1.6915152 ,  1.4246267 ],\n",
       "        [ 1.9637349 ,  0.90921146, -0.16049284, ...,  2.1188207 ,\n",
       "          0.3337893 ,  0.89834803],\n",
       "        [ 2.0315614 ,  1.02377   , -0.00431025, ...,  2.1188207 ,\n",
       "          0.3337893 ,  0.89834803],\n",
       "        ...,\n",
       "        [ 1.5018716 , -0.5380123 , -0.21734512, ...,  2.1187623 ,\n",
       "          0.333735  ,  0.8982975 ],\n",
       "        [ 0.548882  ,  0.37240577, -1.0522068 , ...,  2.118761  ,\n",
       "          0.33373386,  0.8982964 ],\n",
       "        [ 0.12305707,  1.0375065 , -1.8147955 , ...,  2.1187599 ,\n",
       "          0.33373272,  0.8982954 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.pos_embedding(tokenizer([\"d\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'space_segmentation_transformer/encoder/postional_embedding/embedding/embeddings:0' shape=(58, 512) dtype=float32, numpy=\n",
      "array([[ 0.04959753,  0.00386059, -0.04253489, ...,  0.04944536,\n",
      "        -0.02944263, -0.00449242],\n",
      "       [ 0.04138169, -0.02623766,  0.0217234 , ...,  0.01663146,\n",
      "        -0.03474427, -0.01255412],\n",
      "       [ 0.00359427,  0.00391297, -0.00651765, ..., -0.04633972,\n",
      "         0.01852321,  0.00417699],\n",
      "       ...,\n",
      "       [ 0.04425664, -0.00089257,  0.02662169, ...,  0.01721764,\n",
      "        -0.0331916 , -0.04946451],\n",
      "       [-0.03879231, -0.01136684,  0.02757949, ..., -0.03115348,\n",
      "        -0.02648364,  0.04941115],\n",
      "       [ 0.02729204, -0.02474734,  0.04571542, ...,  0.03006331,\n",
      "         0.00380225, -0.00652245]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/query/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[ 6.4165232e-04,  5.6641418e-03,  2.1687355e-03, ...,\n",
      "         -4.5308599e-04, -5.6888640e-04,  1.5458408e-03],\n",
      "        [-3.6441917e-03, -1.4081508e-03,  2.9548178e-03, ...,\n",
      "         -3.9855563e-03,  1.9434138e-03, -5.5245729e-03],\n",
      "        [ 1.3504663e-03, -2.8339156e-03,  7.2460430e-04, ...,\n",
      "         -3.6670072e-03, -4.0817335e-03, -4.8040170e-03]],\n",
      "\n",
      "       [[-4.8715868e-03, -7.3479564e-04,  4.3550213e-03, ...,\n",
      "         -8.0928247e-04, -5.0235675e-03, -9.5250423e-04],\n",
      "        [-6.2721702e-03, -5.2161184e-03, -1.3903045e-03, ...,\n",
      "         -2.0326530e-03,  2.1698738e-03,  1.6950935e-03],\n",
      "        [ 3.3936012e-04, -3.1980071e-03,  5.2113384e-03, ...,\n",
      "         -4.3616132e-03, -2.9904193e-03,  3.7389123e-03]],\n",
      "\n",
      "       [[ 7.0194853e-03,  4.5819008e-03, -2.9985385e-03, ...,\n",
      "         -1.3740021e-03,  6.7444695e-03, -2.2224488e-03],\n",
      "        [-2.5440301e-03,  3.2438945e-03,  4.5607731e-04, ...,\n",
      "         -2.6142576e-03, -2.8725790e-03,  4.5850044e-03],\n",
      "        [-4.3862769e-03,  4.4881180e-03,  1.7573400e-03, ...,\n",
      "         -3.3285804e-03,  8.9672155e-04,  2.0140712e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-3.2307624e-03, -5.6005591e-03, -3.2569980e-04, ...,\n",
      "          2.3665412e-03, -5.0638226e-04,  9.9573529e-04],\n",
      "        [ 7.1910219e-03,  5.2307574e-03, -2.2434846e-03, ...,\n",
      "          2.9036035e-03,  1.1938564e-03,  7.0052980e-03],\n",
      "        [ 5.8590206e-03,  1.9259417e-03,  6.0652336e-03, ...,\n",
      "          4.5826845e-03,  3.4333521e-03,  5.5112815e-03]],\n",
      "\n",
      "       [[-1.5845000e-03, -5.0140894e-03, -5.1088599e-03, ...,\n",
      "         -2.2549953e-03, -4.4604149e-03,  8.4251333e-03],\n",
      "        [ 2.9246660e-03, -1.2331981e-03, -1.4036052e-03, ...,\n",
      "          3.0224898e-03,  1.2436131e-05, -1.7466500e-03],\n",
      "        [ 3.8628990e-03, -1.6303882e-03,  2.1237626e-03, ...,\n",
      "          6.5385405e-04,  6.4933593e-03,  3.3910742e-03]],\n",
      "\n",
      "       [[ 6.7662547e-04, -5.7123289e-03, -5.2085277e-03, ...,\n",
      "         -5.2984226e-03, -4.6540196e-03,  8.8937394e-03],\n",
      "        [ 7.3407963e-03,  4.2783268e-04, -1.5712705e-03, ...,\n",
      "          1.5138228e-03, -5.5638715e-03,  5.7608359e-03],\n",
      "        [ 4.6899454e-03,  2.1651136e-03,  3.9469493e-03, ...,\n",
      "          9.4910624e-04,  3.3743216e-03,  2.0740342e-03]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/query/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[-0.00421887, -0.00515358, -0.00433853, ..., -0.0016819 ,\n",
      "        -0.0050711 ,  0.00445549],\n",
      "       [ 0.00468559,  0.0023192 , -0.00513119, ...,  0.00402839,\n",
      "        -0.0038012 ,  0.00274642],\n",
      "       [ 0.0032582 , -0.00309783,  0.00436844, ...,  0.00435696,\n",
      "         0.00480773,  0.00476746]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/key/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[ 1.0402206e-03,  2.6869290e-03, -7.3602056e-04, ...,\n",
      "         -4.0810020e-04,  3.3757638e-03,  6.3697034e-03],\n",
      "        [ 3.7152090e-03, -2.4335829e-03, -2.1442291e-03, ...,\n",
      "         -2.7035733e-03,  3.3145768e-03,  4.0364280e-04],\n",
      "        [-1.8833884e-03, -1.5280743e-03,  2.2382637e-04, ...,\n",
      "          4.9971752e-03, -6.6522876e-04,  2.4599729e-03]],\n",
      "\n",
      "       [[ 3.5365082e-03,  6.7777378e-03, -9.5886281e-03, ...,\n",
      "          5.6357321e-04, -1.0664981e-03, -7.1270154e-03],\n",
      "        [-7.6054400e-03,  4.5251290e-05, -7.0464239e-03, ...,\n",
      "          3.0062480e-03, -1.9318580e-03,  1.4022057e-03],\n",
      "        [ 2.1606965e-03,  5.8951913e-03,  4.6050767e-04, ...,\n",
      "         -1.5305966e-03,  2.6525566e-03,  6.3891024e-03]],\n",
      "\n",
      "       [[-3.1532240e-03, -2.8434559e-03, -1.5962622e-03, ...,\n",
      "          2.8585172e-03,  9.8021957e-04,  2.1567962e-03],\n",
      "        [-8.6209599e-05,  4.1042096e-03, -5.8231934e-04, ...,\n",
      "          4.6315044e-03, -7.4881689e-05, -1.0759347e-03],\n",
      "        [ 7.6425693e-04,  5.2026002e-04, -3.8149287e-03, ...,\n",
      "          2.3054831e-04, -7.6404493e-03, -4.0376005e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.5322875e-03,  5.0725258e-04, -2.6812050e-03, ...,\n",
      "         -3.8433496e-03, -9.9363015e-04,  2.3256885e-03],\n",
      "        [ 5.9559345e-05, -1.9170851e-03, -4.4594482e-03, ...,\n",
      "          1.0521829e-03, -8.6451266e-03,  3.6711120e-03],\n",
      "        [-5.6600571e-03, -7.8398539e-03,  3.9611622e-03, ...,\n",
      "         -6.9636275e-04, -6.7633705e-04,  2.6793344e-04]],\n",
      "\n",
      "       [[ 2.0529584e-03, -7.2788508e-03,  8.1668617e-03, ...,\n",
      "         -3.6408743e-04,  2.5269826e-04,  3.8727343e-03],\n",
      "        [ 7.3503708e-03,  3.5360549e-03, -6.4492645e-04, ...,\n",
      "         -3.4659200e-03,  5.1400624e-04,  2.8643615e-03],\n",
      "        [-2.2932743e-03, -5.0253118e-03, -2.1273638e-03, ...,\n",
      "          7.5591012e-04, -1.9190052e-03, -2.3113911e-03]],\n",
      "\n",
      "       [[-2.6111489e-03, -1.4950661e-03, -1.6904621e-03, ...,\n",
      "          5.9303758e-03,  2.6170397e-03, -6.8490491e-03],\n",
      "        [-4.0607792e-03,  3.4908897e-03, -2.8856322e-03, ...,\n",
      "         -2.5147763e-03,  2.3855204e-03, -2.6676685e-03],\n",
      "        [ 6.6089840e-04,  1.8009576e-03, -4.6339007e-03, ...,\n",
      "         -2.0165613e-03, -3.3078690e-03,  1.3544972e-04]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/key/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[ 7.8564312e-08, -8.4681439e-08,  1.9124658e-07, ...,\n",
      "         2.0838708e-07, -8.8563841e-09,  2.4245782e-07],\n",
      "       [-1.1546704e-07, -1.7446877e-07,  3.7562209e-08, ...,\n",
      "         3.9027057e-08, -1.7706917e-07,  1.6965099e-07],\n",
      "       [ 1.2261637e-07, -1.4829733e-07, -7.4781283e-08, ...,\n",
      "         1.8322545e-08,  2.2604571e-08, -7.0991526e-08]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/value/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[ 8.38141423e-03, -9.79010016e-03,  7.76875252e-03, ...,\n",
      "         -9.81140044e-03, -6.05820259e-03, -3.75910290e-03],\n",
      "        [ 7.30125699e-03,  2.95447814e-03,  2.78657884e-03, ...,\n",
      "          6.72966149e-03,  8.10236856e-03,  1.76353066e-03],\n",
      "        [-7.91447703e-03,  3.29522160e-03,  5.81211830e-03, ...,\n",
      "         -1.01276329e-02, -1.02862548e-02, -2.41293479e-03]],\n",
      "\n",
      "       [[ 2.42766971e-03, -3.61556630e-03,  8.90364870e-03, ...,\n",
      "         -6.75997930e-03, -6.81927660e-03, -5.06556733e-03],\n",
      "        [ 5.45358611e-03, -4.23147809e-03, -4.45761951e-03, ...,\n",
      "          6.67770952e-03,  4.78900583e-05,  1.00960229e-02],\n",
      "        [-9.56985168e-03,  2.38900166e-03,  2.33639474e-03, ...,\n",
      "         -1.66650466e-03, -7.66652636e-03, -3.29539273e-03]],\n",
      "\n",
      "       [[ 6.79844152e-03, -7.11151538e-03,  1.61415432e-03, ...,\n",
      "         -7.86544383e-03, -5.41936001e-03, -6.85493089e-03],\n",
      "        [ 8.03670660e-03,  7.12987874e-03,  1.73596246e-03, ...,\n",
      "          4.98836255e-03, -6.79251319e-03,  2.65561999e-03],\n",
      "        [-9.10229236e-03,  7.13302940e-03,  8.64318945e-03, ...,\n",
      "         -7.95596186e-03, -7.04949722e-03, -1.95653737e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-6.51937304e-03,  8.77871551e-03, -5.28692082e-03, ...,\n",
      "          8.20154790e-03,  8.00253358e-03,  2.50669941e-03],\n",
      "        [-2.74918182e-03, -3.41406534e-03, -5.92225930e-03, ...,\n",
      "         -6.85752323e-03,  4.02300572e-03, -1.60587835e-03],\n",
      "        [ 4.67981957e-03, -4.61938698e-03, -9.79773421e-03, ...,\n",
      "          1.07231345e-02,  2.94865342e-03,  4.65338584e-03]],\n",
      "\n",
      "       [[-9.48713720e-03,  8.38405173e-03, -3.33194388e-03, ...,\n",
      "          3.22010927e-03,  6.11691782e-03,  4.82292054e-03],\n",
      "        [-9.12460405e-03, -2.94796727e-03, -6.46636076e-03, ...,\n",
      "         -6.25410164e-03,  7.49929203e-03, -8.35455395e-03],\n",
      "        [ 3.34591884e-03, -5.08014951e-03, -1.02883335e-02, ...,\n",
      "          3.61671392e-03,  4.36286069e-03,  3.72218457e-03]],\n",
      "\n",
      "       [[-5.08856447e-03,  2.62280088e-03, -1.61207526e-03, ...,\n",
      "          9.96148121e-03,  8.48612934e-03,  3.28213116e-03],\n",
      "        [-5.34094078e-03, -7.56951934e-03, -9.88431182e-03, ...,\n",
      "         -7.23139429e-03,  6.87924726e-03, -6.96090516e-03],\n",
      "        [ 1.07285054e-02, -1.04204109e-02, -6.69557275e-03, ...,\n",
      "          6.15948904e-03,  5.76687185e-03,  6.10366184e-03]]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/value/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[-0.00600496,  0.00600013, -0.00600402, ...,  0.00600506,\n",
      "         0.00600481,  0.00600495],\n",
      "       [-0.00600498, -0.00600302, -0.00600234, ..., -0.00600474,\n",
      "         0.00594911, -0.00600349],\n",
      "       [ 0.00600402, -0.00600494, -0.00600049, ...,  0.00600494,\n",
      "         0.00600491,  0.00600446]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/attention_output/kernel:0' shape=(3, 512, 512) dtype=float32, numpy=\n",
      "array([[[ 0.02830022, -0.03074998,  0.02126125, ..., -0.03668   ,\n",
      "          0.02038301,  0.02102349],\n",
      "        [ 0.0084244 , -0.02238587, -0.01402087, ..., -0.03295227,\n",
      "          0.00177215,  0.02161143],\n",
      "        [-0.0353843 ,  0.01855292,  0.00948968, ..., -0.03680314,\n",
      "         -0.00098013, -0.01044376],\n",
      "        ...,\n",
      "        [-0.01444886, -0.04622532, -0.01690462, ...,  0.0219207 ,\n",
      "          0.03540734, -0.01745202],\n",
      "        [-0.01997763, -0.0068932 , -0.0106779 , ...,  0.00208147,\n",
      "          0.00304961,  0.01350855],\n",
      "        [-0.04126091,  0.01268078,  0.01525141, ...,  0.04509997,\n",
      "          0.00688773,  0.03057439]],\n",
      "\n",
      "       [[ 0.00355253,  0.02459314,  0.01140345, ..., -0.01968019,\n",
      "         -0.01232026, -0.00830121],\n",
      "        [ 0.02840673,  0.03295012, -0.00508379, ..., -0.00999132,\n",
      "          0.04167134,  0.04229306],\n",
      "        [-0.0494498 ,  0.03726173, -0.00798131, ...,  0.01656881,\n",
      "         -0.02221115, -0.0248454 ],\n",
      "        ...,\n",
      "        [ 0.02161302,  0.01690298, -0.04192623, ..., -0.01812816,\n",
      "          0.00733433,  0.00358453],\n",
      "        [ 0.00103601, -0.02397412,  0.0317519 , ...,  0.02934058,\n",
      "         -0.02748538, -0.03354188],\n",
      "        [ 0.00415126,  0.03230918, -0.02653537, ..., -0.02515152,\n",
      "          0.03483166, -0.03965108]],\n",
      "\n",
      "       [[-0.01102989,  0.02186429,  0.01872197, ...,  0.01606652,\n",
      "         -0.03250465, -0.03927048],\n",
      "        [ 0.02948673, -0.00905225,  0.00644958, ...,  0.03485072,\n",
      "          0.02056332, -0.01420848],\n",
      "        [ 0.0022087 , -0.01173388, -0.03081427, ...,  0.02110825,\n",
      "         -0.00475736,  0.0067932 ],\n",
      "        ...,\n",
      "        [ 0.01923383,  0.01178318,  0.02170573, ...,  0.01541547,\n",
      "          0.00510986, -0.00294351],\n",
      "        [-0.02994444,  0.01995349,  0.04294375, ..., -0.03775052,\n",
      "         -0.01026543, -0.01792171],\n",
      "        [-0.00446725,  0.02523957,  0.00410449, ..., -0.00610285,\n",
      "         -0.01504232,  0.04559979]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/multi_head_attention/attention_output/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600419, -0.00600459, -0.00600438,  0.00600519,  0.00600507,\n",
      "       -0.00600363,  0.00600474, -0.00600499,  0.00600456, -0.00600384,\n",
      "       -0.00600503,  0.00600509,  0.00600466,  0.00600501, -0.00600452,\n",
      "        0.00600455, -0.00600409,  0.00600473, -0.00600484,  0.00600421,\n",
      "        0.00600338, -0.00600483,  0.00600386, -0.00600481, -0.00600221,\n",
      "       -0.0060051 , -0.00600527,  0.00600511,  0.00600518, -0.00600527,\n",
      "       -0.00600456, -0.00596511,  0.00600488,  0.00600252, -0.00600455,\n",
      "       -0.0060052 , -0.00600517, -0.00600433,  0.00600523,  0.00600537,\n",
      "        0.00600532, -0.00600536, -0.00600523,  0.00600533, -0.00600525,\n",
      "       -0.00600523,  0.006004  ,  0.00600453,  0.0060047 , -0.0059883 ,\n",
      "        0.00600462,  0.00600524,  0.00600309, -0.00600517, -0.00600513,\n",
      "       -0.00600475,  0.00598541,  0.00600418, -0.0060018 , -0.00600503,\n",
      "       -0.00600269, -0.00600435,  0.00600532,  0.00600051,  0.00600527,\n",
      "       -0.00600514,  0.00600511,  0.00600542, -0.00600521,  0.00600534,\n",
      "        0.00600497, -0.00600533,  0.00600481, -0.00600416,  0.00600534,\n",
      "       -0.00600517, -0.00600515, -0.00600076, -0.00600364, -0.00600273,\n",
      "        0.00600388,  0.00600394,  0.00600405,  0.00599957, -0.00600259,\n",
      "        0.00600543,  0.00600498,  0.00600465,  0.00600486, -0.00600263,\n",
      "        0.00600505,  0.00599881,  0.00600539, -0.00600508,  0.00600533,\n",
      "        0.00600529,  0.00600491, -0.0060052 ,  0.00600522,  0.00600532,\n",
      "       -0.00600511, -0.00600505, -0.00600509, -0.00600475,  0.00600532,\n",
      "        0.006005  , -0.0060035 ,  0.0060051 , -0.00600536, -0.00600531,\n",
      "        0.00600415,  0.0060039 , -0.0060023 , -0.00598899,  0.00600518,\n",
      "        0.00600474,  0.00600415, -0.00600289,  0.00600442,  0.00600164,\n",
      "        0.00600536,  0.00600067,  0.00600362,  0.0060047 , -0.00600488,\n",
      "        0.00600511,  0.00600511, -0.00600478, -0.00600495, -0.00600525,\n",
      "        0.00600533, -0.00600507, -0.00598594,  0.00600499,  0.00600519,\n",
      "       -0.00600525,  0.00600526, -0.00600524,  0.00600496,  0.00600531,\n",
      "       -0.00600296, -0.006005  , -0.0060024 ,  0.00600523,  0.00600387,\n",
      "       -0.00600535,  0.00600507,  0.0060051 , -0.0059894 ,  0.00600443,\n",
      "       -0.00600501,  0.00600522, -0.00600489,  0.00600474,  0.00600125,\n",
      "       -0.00600488, -0.00600482, -0.00600519, -0.00600529,  0.00600457,\n",
      "       -0.00600337, -0.00600519, -0.00600463, -0.00600414, -0.00600474,\n",
      "       -0.00599966,  0.00600461, -0.00600513, -0.00600489, -0.0060052 ,\n",
      "       -0.00600527, -0.00600532, -0.00600455, -0.00600534,  0.00600073,\n",
      "       -0.00600171,  0.00600526, -0.00600203,  0.00600519, -0.00600467,\n",
      "       -0.00600505, -0.00600504,  0.00600425,  0.00600526, -0.00600446,\n",
      "       -0.0060053 ,  0.00600131, -0.00600527, -0.00599311,  0.0059961 ,\n",
      "       -0.00600323, -0.00600521,  0.00600511, -0.00600489,  0.0060049 ,\n",
      "        0.00600506,  0.00600384,  0.00600448,  0.00600525,  0.00600512,\n",
      "       -0.00600369,  0.00599915,  0.00600536,  0.00600488,  0.00600408,\n",
      "       -0.00600499, -0.00600491, -0.00600475,  0.00600505,  0.00599456,\n",
      "       -0.00600495, -0.00600535, -0.00600314, -0.00600528,  0.0060053 ,\n",
      "       -0.0060051 , -0.00600518, -0.00600386, -0.00600459,  0.00600525,\n",
      "        0.00600514, -0.00600514, -0.00600379, -0.00599474,  0.00599384,\n",
      "        0.00600535,  0.00600518,  0.00600527, -0.00600392, -0.00600469,\n",
      "        0.00600518, -0.00600084, -0.00600447, -0.00600533,  0.00600511,\n",
      "       -0.00600488,  0.00589077, -0.00600372, -0.00600338,  0.00600531,\n",
      "        0.00600438,  0.00600532, -0.00600526, -0.00600518, -0.00600493,\n",
      "       -0.00600484, -0.0059872 ,  0.00600475,  0.00600501,  0.00600523,\n",
      "       -0.00600237,  0.00600527, -0.00600421, -0.00600509,  0.00600537,\n",
      "       -0.00600504, -0.00594385,  0.00600419, -0.0060049 ,  0.00600536,\n",
      "        0.00600494,  0.00600525,  0.00600418, -0.00600519,  0.00600514,\n",
      "        0.00600513,  0.00600456, -0.00598918, -0.00600529,  0.00600114,\n",
      "        0.00600427, -0.00600509, -0.00600426, -0.00600512, -0.00600364,\n",
      "        0.00600515,  0.00600448, -0.00600529,  0.00600446,  0.00600429,\n",
      "       -0.00600303,  0.00600506,  0.00600498, -0.00600509,  0.00600283,\n",
      "       -0.00600466, -0.00600379,  0.00600504,  0.00600458, -0.00600521,\n",
      "        0.00600535, -0.00600463,  0.00599423,  0.00600248,  0.00600488,\n",
      "       -0.00600506, -0.00600499,  0.0060053 , -0.00600527, -0.00600335,\n",
      "       -0.00600489,  0.00598851,  0.00600525,  0.00600509, -0.00600499,\n",
      "       -0.00600518,  0.00600525,  0.00600534,  0.0060017 , -0.00600527,\n",
      "        0.00600518, -0.00600491, -0.00600526, -0.00600502, -0.00600515,\n",
      "        0.00600489,  0.00600529,  0.00600531, -0.00600507,  0.00600502,\n",
      "        0.00600385, -0.0060051 ,  0.00600518,  0.00600397,  0.00600527,\n",
      "       -0.00600526,  0.00600529,  0.00600507,  0.0060033 ,  0.00600509,\n",
      "       -0.00600499, -0.00600494,  0.00600475,  0.00600484, -0.00599641,\n",
      "       -0.00600486,  0.00600523, -0.00600513, -0.00600514,  0.00600335,\n",
      "       -0.00600513,  0.00599817,  0.00600494,  0.00600335,  0.00600404,\n",
      "       -0.0060052 ,  0.00600433, -0.00600496, -0.00600347,  0.00600504,\n",
      "        0.00600527, -0.00600521,  0.00600519,  0.00600513, -0.00600079,\n",
      "       -0.00600542,  0.00600516, -0.00599896, -0.00599333, -0.00600522,\n",
      "        0.00600451, -0.00600241,  0.00600525, -0.00600531,  0.00600482,\n",
      "        0.00600476, -0.00600482, -0.0060053 , -0.00600508, -0.00600538,\n",
      "       -0.00600536,  0.00600506,  0.00600491,  0.00600365,  0.00600509,\n",
      "        0.0060045 , -0.00600458,  0.00598602,  0.00600527, -0.00600436,\n",
      "        0.00600465, -0.00600496,  0.00600498, -0.0060053 , -0.00600539,\n",
      "        0.00600333, -0.00600534, -0.00600478,  0.0060034 , -0.00600542,\n",
      "        0.00600533,  0.00600516,  0.00600493, -0.00600492, -0.00600511,\n",
      "       -0.006005  , -0.00600502, -0.00600414, -0.00600489, -0.00600268,\n",
      "       -0.00600486, -0.00600483,  0.00600254, -0.00600499,  0.00600458,\n",
      "        0.00600479, -0.00600528,  0.00600532,  0.00600436,  0.00600509,\n",
      "        0.00600534,  0.00600519,  0.00600403,  0.00600261,  0.00600527,\n",
      "        0.00600534, -0.00600534, -0.00600504, -0.00600485,  0.00600543,\n",
      "       -0.00600399, -0.00600533,  0.00600529,  0.00592925,  0.00600515,\n",
      "       -0.0060035 , -0.00600526,  0.00600463, -0.00600523, -0.00600532,\n",
      "        0.00598315,  0.00599513, -0.00600483, -0.00600538,  0.00599974,\n",
      "       -0.00600493,  0.00600432, -0.00600475, -0.0060053 ,  0.00600522,\n",
      "       -0.0060029 , -0.00600499,  0.00600526, -0.00600275, -0.00600512,\n",
      "        0.00600247, -0.00600284,  0.0060031 , -0.00600501,  0.00600462,\n",
      "       -0.00600502, -0.00600248, -0.00600505,  0.00600529, -0.00600534,\n",
      "       -0.00600526,  0.0060053 , -0.00600541, -0.00600334,  0.00599591,\n",
      "       -0.00600494,  0.00600459, -0.00599884, -0.00599847,  0.006005  ,\n",
      "       -0.0060053 , -0.00600504, -0.00600496, -0.00600509, -0.00600534,\n",
      "        0.00600406,  0.0060041 , -0.00600525, -0.0060051 , -0.00600485,\n",
      "       -0.00600539,  0.00600521,  0.0060014 , -0.006005  ,  0.00600516,\n",
      "       -0.00600495,  0.00600472,  0.00600509,  0.00600529,  0.00600491,\n",
      "        0.00600331,  0.00600487,  0.00600533, -0.00600521, -0.00600517,\n",
      "       -0.00600283,  0.00600488,  0.00600533, -0.00600431,  0.00600505,\n",
      "       -0.00600494, -0.00600505,  0.00600473, -0.00600027,  0.00600439,\n",
      "        0.00600492, -0.006005  , -0.00600521,  0.00600477,  0.00600439,\n",
      "       -0.00600389, -0.00600347, -0.00600342,  0.00600412,  0.00600521,\n",
      "       -0.00600435, -0.00600277], dtype=float32)>\n",
      "<tf.Variable 'dense/kernel:0' shape=(512, 1028) dtype=float32, numpy=\n",
      "array([[-0.03785023, -0.02924876,  0.00472706, ..., -0.01496582,\n",
      "        -0.05266029,  0.01040391],\n",
      "       [-0.03077903,  0.02596993, -0.0137281 , ..., -0.02820219,\n",
      "        -0.0231797 , -0.01227541],\n",
      "       [-0.0398076 ,  0.02881603,  0.02819165, ...,  0.05107621,\n",
      "        -0.06307004, -0.04411518],\n",
      "       ...,\n",
      "       [ 0.01572755,  0.0339215 , -0.0518099 , ...,  0.06321494,\n",
      "         0.0434496 ,  0.04651589],\n",
      "       [ 0.04600724,  0.03695495, -0.00723235, ...,  0.04907718,\n",
      "        -0.02386029,  0.03903745],\n",
      "       [ 0.03295642, -0.05241342,  0.01930415, ..., -0.04594408,\n",
      "        -0.02661351, -0.04492947]], dtype=float32)>\n",
      "<tf.Variable 'dense/bias:0' shape=(1028,) dtype=float32, numpy=\n",
      "array([ 0.00600347,  0.00600427, -0.0059971 , ...,  0.00600148,\n",
      "        0.00600448, -0.00600306], dtype=float32)>\n",
      "<tf.Variable 'dense_1/kernel:0' shape=(1028, 512) dtype=float32, numpy=\n",
      "array([[ 0.05199693,  0.00981732, -0.06257294, ...,  0.02515936,\n",
      "         0.05636273, -0.03352273],\n",
      "       [ 0.04441676, -0.03626969, -0.03324359, ...,  0.02549259,\n",
      "         0.03493976,  0.00895472],\n",
      "       [ 0.00299396,  0.0224467 , -0.02515825, ...,  0.05243402,\n",
      "         0.06704158, -0.00927746],\n",
      "       ...,\n",
      "       [ 0.05021888, -0.03491472, -0.03729977, ..., -0.02482322,\n",
      "         0.0359619 ,  0.00288165],\n",
      "       [-0.02888453,  0.0511733 ,  0.0095598 , ...,  0.0628421 ,\n",
      "        -0.02394408, -0.04630113],\n",
      "       [-0.04716465, -0.00568569,  0.02866155, ...,  0.00480899,\n",
      "         0.03059984,  0.03727822]], dtype=float32)>\n",
      "<tf.Variable 'dense_1/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600433, -0.00600458, -0.00575919,  0.00600523,  0.00600496,\n",
      "       -0.00600305,  0.00600507, -0.0060042 ,  0.00600351,  0.00600061,\n",
      "       -0.00600492,  0.0060045 , -0.00597707,  0.00600508, -0.00600511,\n",
      "       -0.00600461, -0.00600417,  0.00600455, -0.00600474,  0.00600155,\n",
      "        0.00600463, -0.00599729,  0.00600474, -0.00599821, -0.00600469,\n",
      "       -0.00600459, -0.00600535,  0.006005  ,  0.00600521, -0.00600527,\n",
      "       -0.00600439,  0.00600506, -0.00600462, -0.00600425,  0.00600422,\n",
      "       -0.00600493, -0.00600518,  0.00599334,  0.00600509,  0.00600533,\n",
      "        0.00600519, -0.00600535, -0.00600513,  0.00600501, -0.00600515,\n",
      "       -0.0060052 , -0.00600147,  0.00600501,  0.00600432, -0.00600474,\n",
      "        0.00600515,  0.0060049 , -0.00600504, -0.00600476, -0.00600516,\n",
      "       -0.00600437, -0.00600149, -0.00600438, -0.00600441, -0.0060049 ,\n",
      "        0.00599864, -0.00600289,  0.00600523, -0.00600509,  0.00600525,\n",
      "       -0.00600404,  0.00600469,  0.00600541, -0.00600477,  0.00600533,\n",
      "        0.0060034 , -0.00600514, -0.00600474, -0.0060037 ,  0.00600532,\n",
      "       -0.00600349, -0.00600511, -0.00600248, -0.00600487,  0.00600384,\n",
      "        0.00600125, -0.00600388,  0.00600502,  0.00600406,  0.00600426,\n",
      "        0.00600537,  0.00600485,  0.00600453,  0.00600494,  0.00600497,\n",
      "        0.00600404,  0.00600501,  0.00600533, -0.00600236,  0.00600531,\n",
      "        0.00600507,  0.00600413, -0.00600449,  0.00600454,  0.00600535,\n",
      "       -0.00600505,  0.00600216, -0.00600446, -0.00600348,  0.00600535,\n",
      "        0.00600509, -0.00600459,  0.0060052 , -0.00600537, -0.00600534,\n",
      "        0.00600408,  0.00600505, -0.00599845,  0.00600425,  0.00600524,\n",
      "        0.00600498,  0.00600263,  0.00600479,  0.00599715,  0.00595465,\n",
      "        0.00600529, -0.0060037 , -0.00600396,  0.0060048 ,  0.00600013,\n",
      "        0.00600509,  0.00600506, -0.00600482, -0.00600496, -0.00600522,\n",
      "        0.00600526, -0.00600479, -0.0060039 ,  0.00600424,  0.0060048 ,\n",
      "       -0.00600523,  0.0060052 , -0.00600459,  0.00544622,  0.00600532,\n",
      "       -0.00600278, -0.00600515, -0.00600437,  0.00600516, -0.0060043 ,\n",
      "       -0.0060051 ,  0.00600469,  0.00600517,  0.00600377, -0.00600292,\n",
      "       -0.00600515,  0.00600515, -0.00600438,  0.00600491, -0.00600161,\n",
      "       -0.00600503, -0.00600525, -0.00600518, -0.00600505,  0.00600476,\n",
      "       -0.0060051 , -0.00600518, -0.00600362, -0.00600318, -0.006005  ,\n",
      "       -0.00599991,  0.0059996 , -0.00600527, -0.00600514, -0.00600523,\n",
      "       -0.00600522, -0.00600506, -0.0060043 , -0.00600535, -0.00600521,\n",
      "       -0.00600321,  0.00600515, -0.00599261,  0.00600519, -0.00597225,\n",
      "       -0.00600449, -0.00600527,  0.00600467,  0.00600526, -0.00600319,\n",
      "       -0.00600528, -0.00600517, -0.00600515,  0.00600474,  0.0060046 ,\n",
      "        0.00600474, -0.00600524,  0.00600511, -0.00600485,  0.00600455,\n",
      "        0.00600485,  0.00600124,  0.00600422,  0.00600527,  0.00600498,\n",
      "        0.00600193, -0.00593106,  0.00600534,  0.00600344,  0.00600497,\n",
      "        0.00600136,  0.00600045, -0.00600397,  0.0059532 , -0.00600472,\n",
      "        0.00599865, -0.00600521,  0.00600359, -0.00600515,  0.0060053 ,\n",
      "       -0.00600487, -0.00600284, -0.006005  ,  0.00599073,  0.00600519,\n",
      "        0.00600525, -0.00600479, -0.00600454, -0.00600282,  0.00600422,\n",
      "        0.00600536,  0.00600316,  0.00600507, -0.00600429, -0.00600436,\n",
      "        0.00600502, -0.00600515, -0.00599976, -0.00600525,  0.00600504,\n",
      "       -0.00600489, -0.00599885, -0.00600493, -0.00600485,  0.00600525,\n",
      "       -0.00600246,  0.00600539, -0.00600529, -0.00600529, -0.00600499,\n",
      "       -0.00600521,  0.00599283,  0.00600499,  0.00600488,  0.00600477,\n",
      "        0.00600286,  0.00600525, -0.00600523, -0.00600526,  0.00600519,\n",
      "       -0.00600519,  0.00600336, -0.00600481, -0.00600401,  0.00600517,\n",
      "        0.00600311,  0.00599466,  0.0060025 , -0.00600504,  0.00600521,\n",
      "        0.00600484,  0.00600372, -0.00600261, -0.00600505, -0.00579407,\n",
      "       -0.00600446, -0.00600498, -0.00600339, -0.00600457, -0.00599432,\n",
      "        0.00600527, -0.0060044 , -0.00600511,  0.00600115,  0.00600507,\n",
      "        0.00600425, -0.0060017 ,  0.00600525, -0.00600458,  0.00600486,\n",
      "       -0.00600138, -0.00556303,  0.00600517,  0.00600417, -0.00600514,\n",
      "        0.00600534, -0.00600507, -0.00600334, -0.00599817,  0.00600501,\n",
      "       -0.00600527, -0.00600495,  0.00600379, -0.00600516,  0.00600271,\n",
      "       -0.00600522, -0.00600424,  0.00600521,  0.00600486, -0.00600508,\n",
      "       -0.00600503,  0.00600524,  0.00600531,  0.0060046 , -0.0060051 ,\n",
      "        0.00600515, -0.00600506, -0.006005  , -0.00600492, -0.00600514,\n",
      "        0.00600492,  0.00600468,  0.00600494, -0.00600505,  0.00600488,\n",
      "        0.00600455, -0.0060051 ,  0.00600479,  0.00600251,  0.00600527,\n",
      "       -0.00600535,  0.00600516,  0.00600504,  0.00600417,  0.00600521,\n",
      "        0.00597964, -0.00599953,  0.00600524,  0.00600424, -0.00600457,\n",
      "       -0.00600503,  0.00600457, -0.00600412, -0.00600493, -0.00600462,\n",
      "       -0.00600504, -0.00600305,  0.00600491,  0.00600397,  0.00600488,\n",
      "       -0.0060052 ,  0.00600474, -0.00600514, -0.0058863 ,  0.00600488,\n",
      "        0.00600518, -0.00600507,  0.0060051 ,  0.00600508, -0.00600524,\n",
      "       -0.0060054 ,  0.00600312,  0.00600469,  0.00600452, -0.00600441,\n",
      "        0.00600418,  0.0060049 ,  0.00600535, -0.00600498,  0.0060046 ,\n",
      "        0.00600507, -0.00600477, -0.00600521, -0.0060017 , -0.00600531,\n",
      "       -0.00600518,  0.00600429,  0.0060051 , -0.00600371,  0.00600406,\n",
      "        0.00600509, -0.0060046 ,  0.00600387,  0.00600498,  0.00600472,\n",
      "       -0.00600262, -0.00600496,  0.00600361, -0.00600521, -0.00600534,\n",
      "       -0.00599853, -0.00600534, -0.00600507, -0.00600261, -0.00600539,\n",
      "        0.00600529,  0.00599751,  0.00600418, -0.00600494, -0.006005  ,\n",
      "        0.00600357, -0.00600454, -0.00600411, -0.00600268,  0.00600452,\n",
      "       -0.0060034 , -0.00600344, -0.00600433, -0.00600459,  0.00600486,\n",
      "       -0.00600434, -0.00600475,  0.00600523,  0.00600346,  0.00600515,\n",
      "        0.00600523,  0.00600488,  0.00599685,  0.00600491,  0.00600503,\n",
      "        0.0060053 , -0.00600526,  0.00600297, -0.00600519,  0.00600541,\n",
      "        0.0060046 , -0.00600534,  0.00600521,  0.00600388,  0.00600473,\n",
      "       -0.00600485, -0.00600528,  0.00600494, -0.00600524, -0.00600528,\n",
      "       -0.00600461, -0.00581567,  0.00600207, -0.00600523,  0.00600466,\n",
      "        0.00599875,  0.00600451, -0.00600444, -0.00600519,  0.006005  ,\n",
      "       -0.00598016, -0.00600484,  0.00600478,  0.0060045 , -0.00600439,\n",
      "       -0.00600434, -0.00600425,  0.00600501, -0.00600397,  0.0060044 ,\n",
      "       -0.00600505,  0.00600456, -0.00600525,  0.00600501, -0.00600528,\n",
      "       -0.00600506,  0.00600521, -0.00600533, -0.00600156, -0.00600505,\n",
      "       -0.0060049 ,  0.00600515,  0.00600421,  0.00600451,  0.0060051 ,\n",
      "       -0.00600528, -0.00600365,  0.00600198, -0.00600462, -0.00600528,\n",
      "       -0.00600447,  0.00600491, -0.00600522, -0.00600493, -0.00599833,\n",
      "       -0.00600536,  0.00600464,  0.00600485, -0.00600508,  0.00600497,\n",
      "       -0.0060048 ,  0.00599998,  0.00600521,  0.00600511,  0.00600499,\n",
      "        0.00600403,  0.006005  ,  0.00600534, -0.00600496, -0.00600517,\n",
      "       -0.00600208,  0.0060051 ,  0.00600529, -0.00600487,  0.00600484,\n",
      "       -0.00600479, -0.00600442,  0.00599566,  0.00600485, -0.00600304,\n",
      "        0.00600425, -0.00600497, -0.00600436,  0.00600526,  0.00600424,\n",
      "        0.00600261, -0.00600508,  0.00599799,  0.00599628,  0.0060045 ,\n",
      "        0.00600324, -0.00600434], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([1.0060028 , 0.9940275 , 1.0060004 , 0.993998  , 0.99399495,\n",
      "       1.006004  , 0.99400157, 1.0060029 , 0.994007  , 1.0060031 ,\n",
      "       1.0060034 , 0.99399745, 0.99399793, 0.993995  , 1.0060014 ,\n",
      "       0.9939972 , 1.0060031 , 0.99399817, 1.0057445 , 1.006     ,\n",
      "       1.0054111 , 1.006002  , 0.9940133 , 0.99399805, 1.0060024 ,\n",
      "       1.0060043 , 1.0060048 , 0.9939951 , 0.993998  , 1.0060046 ,\n",
      "       1.0060024 , 1.0059956 , 0.9939962 , 1.0059948 , 1.0060027 ,\n",
      "       1.0060046 , 1.005868  , 1.0060024 , 0.9939958 , 0.99399483,\n",
      "       0.99399585, 1.0060047 , 1.0060011 , 0.99399525, 1.0060022 ,\n",
      "       1.0060045 , 0.994001  , 1.0024462 , 0.99399644, 0.99399745,\n",
      "       0.9939992 , 0.9939966 , 0.9940332 , 0.9940017 , 1.0060042 ,\n",
      "       1.0060042 , 0.9939988 , 0.99399716, 1.0060029 , 1.0060045 ,\n",
      "       1.0060034 , 0.9939989 , 0.9939955 , 1.0059137 , 1.0060014 ,\n",
      "       1.0060046 , 0.99399555, 0.9939951 , 1.0060016 , 0.9939953 ,\n",
      "       0.9939989 , 1.0060029 , 0.99399585, 1.0060004 , 0.9939954 ,\n",
      "       1.0060037 , 1.0060028 , 1.0060024 , 1.0060022 , 1.006003  ,\n",
      "       0.99399644, 1.0060023 , 1.0060003 , 0.9939979 , 1.0060002 ,\n",
      "       1.0060024 , 1.0060004 , 0.9939965 , 0.9939958 , 1.0060022 ,\n",
      "       0.9939994 , 1.0060006 , 1.0060045 , 0.9939978 , 1.0060047 ,\n",
      "       1.0060048 , 1.0060021 , 0.9939951 , 1.0060045 , 1.0060045 ,\n",
      "       1.0059879 , 0.9939961 , 0.9939962 , 0.9939957 , 1.0060045 ,\n",
      "       1.0060042 , 0.99399817, 1.0059992 , 0.99399555, 0.99399585,\n",
      "       1.0060011 , 1.006     , 0.99406314, 1.006     , 1.0060042 ,\n",
      "       0.9939965 , 1.0059961 , 1.0059993 , 1.0060042 , 0.994002  ,\n",
      "       1.0060025 , 1.0059966 , 1.0059987 , 1.0060034 , 1.0059437 ,\n",
      "       1.0060042 , 1.0060029 , 1.0060042 , 0.99403614, 0.99399537,\n",
      "       0.9939978 , 1.0060042 , 1.0059998 , 1.0059837 , 0.9939957 ,\n",
      "       0.99399555, 1.0060045 , 1.0060014 , 1.0060003 , 0.99400234,\n",
      "       1.0059944 , 1.0060022 , 1.0059963 , 0.9939953 , 1.0060034 ,\n",
      "       1.0060045 , 0.994     , 0.9940262 , 1.0059543 , 1.0060029 ,\n",
      "       0.99401075, 0.99399793, 1.0060028 , 0.99400616, 1.006     ,\n",
      "       1.0060034 , 1.0059986 , 1.0060042 , 0.994002  , 0.99399793,\n",
      "       0.9940152 , 1.0060025 , 1.0060014 , 1.0060029 , 1.0060014 ,\n",
      "       0.9940997 , 1.0059996 , 1.0060033 , 1.0060004 , 1.0060025 ,\n",
      "       1.0060045 , 0.9939957 , 1.006004  , 1.0060045 , 0.9939979 ,\n",
      "       0.99399835, 0.9939971 , 1.0060004 , 0.9939958 , 0.99421096,\n",
      "       1.0060027 , 1.0060034 , 0.9939965 , 0.9939951 , 1.0060033 ,\n",
      "       1.0060048 , 0.9939976 , 1.0060045 , 1.0059745 , 1.0059805 ,\n",
      "       1.0060014 , 1.0060045 , 0.99400336, 1.0060043 , 0.99399585,\n",
      "       0.99400145, 0.99399924, 0.9939961 , 0.9939966 , 0.99399537,\n",
      "       1.0060029 , 0.99400157, 0.99399513, 0.99399644, 1.0060016 ,\n",
      "       1.0060042 , 1.0060042 , 1.0060031 , 0.9939965 , 1.0059916 ,\n",
      "       1.0060042 , 1.0060047 , 1.0060029 , 1.0060042 , 0.99399513,\n",
      "       1.0060045 , 1.0060046 , 0.99401283, 1.0059916 , 0.99399495,\n",
      "       1.0060029 , 1.0060042 , 1.0060022 , 1.0059986 , 1.0059978 ,\n",
      "       0.9939948 , 0.99399537, 0.9939961 , 0.9940595 , 1.0060043 ,\n",
      "       0.99399555, 1.0060034 , 1.0060042 , 1.0060043 , 0.99399537,\n",
      "       1.0060042 , 1.0059999 , 1.0059975 , 1.006002  , 0.99399555,\n",
      "       0.9940008 , 0.993998  , 1.006004  , 1.0060045 , 1.0060029 ,\n",
      "       1.0060043 , 1.0060014 , 0.99399537, 1.0059167 , 0.99399495,\n",
      "       1.006     , 0.99399555, 1.0060021 , 1.0060029 , 0.993995  ,\n",
      "       1.0060042 , 1.0060042 , 0.9940092 , 1.0060045 , 0.99399626,\n",
      "       0.99399585, 0.99399513, 0.9939994 , 1.0060042 , 0.9939955 ,\n",
      "       0.99399805, 0.9939975 , 1.0060004 , 1.0060045 , 0.9939995 ,\n",
      "       1.005945  , 1.0060045 , 1.0059927 , 1.0060042 , 0.99400413,\n",
      "       0.9939961 , 0.99399614, 1.0060045 , 0.9939985 , 0.99399817,\n",
      "       1.006004  , 0.9939956 , 0.9939956 , 1.0060045 , 1.0059998 ,\n",
      "       1.0060029 , 0.9940171 , 0.9939965 , 0.9939988 , 1.0060048 ,\n",
      "       0.99399483, 1.006004  , 1.0060028 , 1.0059993 , 0.9940024 ,\n",
      "       1.0060029 , 1.0060045 , 0.99399585, 1.0060045 , 0.9940287 ,\n",
      "       0.9940084 , 1.0060003 , 0.9939994 , 0.99399537, 1.0060042 ,\n",
      "       1.0060043 , 0.99399513, 0.99399495, 1.0060022 , 1.0060024 ,\n",
      "       0.9939956 , 1.0060024 , 1.0060045 , 1.0060018 , 1.0060045 ,\n",
      "       0.9939989 , 0.99399644, 0.99399483, 1.0060042 , 0.9939952 ,\n",
      "       1.0060014 , 1.0060043 , 0.9939953 , 0.99399644, 0.9939951 ,\n",
      "       1.0060045 , 0.9939965 , 0.993996  , 1.0059816 , 0.99400556,\n",
      "       1.0060029 , 1.0060034 , 0.99399585, 0.99399644, 0.9940111 ,\n",
      "       1.0060045 , 0.99399537, 1.0060043 , 1.0060045 , 1.0060029 ,\n",
      "       1.0060045 , 0.99400324, 0.9939957 , 0.9939962 , 0.9939989 ,\n",
      "       1.0060046 , 0.9939986 , 1.0060042 , 0.9940104 , 0.99399793,\n",
      "       0.9939951 , 1.0060045 , 0.9939972 , 0.99399817, 1.0059954 ,\n",
      "       1.0060046 , 0.99399537, 1.0060028 , 0.9940011 , 1.0060043 ,\n",
      "       0.99399555, 1.0060033 , 1.0060045 , 0.9939971 , 1.0060034 ,\n",
      "       1.0060037 , 0.9939956 , 0.9939983 , 1.0059966 , 0.99399525,\n",
      "       0.993995  , 1.0060014 , 0.9939986 , 0.99400425, 1.0060042 ,\n",
      "       1.0060037 , 0.99399674, 0.99399716, 1.0060046 , 0.99399537,\n",
      "       1.0060042 , 0.9939958 , 1.0060034 , 0.9939949 , 0.9939947 ,\n",
      "       0.99400187, 0.99399483, 0.9939956 , 1.0060042 , 0.9939947 ,\n",
      "       1.0060034 , 1.0060046 , 1.0060045 , 0.99399585, 0.99399513,\n",
      "       0.9939957 , 0.9939951 , 0.99399686, 0.9939962 , 0.99484205,\n",
      "       0.9939951 , 0.99399817, 1.0060029 , 0.993995  , 1.0060034 ,\n",
      "       1.0060042 , 0.9939951 , 1.0060047 , 1.0060041 , 1.0060047 ,\n",
      "       1.0060048 , 1.0060048 , 1.0060029 , 1.0060027 , 1.0060048 ,\n",
      "       1.0060048 , 0.9939949 , 0.99399495, 0.9939956 , 1.0060049 ,\n",
      "       0.99399644, 0.99399495, 1.0060048 , 1.0059329 , 1.0060047 ,\n",
      "       0.9940003 , 0.9939949 , 1.0060034 , 0.993995  , 0.99399495,\n",
      "       1.0060014 , 1.005896  , 0.9939958 , 0.99399483, 1.0059919 ,\n",
      "       0.993995  , 1.0060042 , 0.99399716, 0.99399483, 1.0060046 ,\n",
      "       0.994002  , 0.9939951 , 1.0060047 , 0.99399817, 0.9939958 ,\n",
      "       1.0060014 , 0.9939979 , 1.0060033 , 0.9939955 , 1.0060045 ,\n",
      "       0.9939956 , 0.99399674, 0.99399537, 1.0060045 , 0.9939948 ,\n",
      "       0.99399495, 1.0060048 , 0.9939951 , 0.99399745, 1.0060034 ,\n",
      "       0.99399513, 1.0060042 , 1.0060028 , 0.9939996 , 1.0060043 ,\n",
      "       0.99399495, 0.99399537, 0.99399537, 0.9939952 , 0.9939947 ,\n",
      "       1.0060034 , 1.0060029 , 0.99399495, 0.99399525, 0.9939951 ,\n",
      "       0.9939947 , 1.0060047 , 1.006004  , 0.993996  , 1.0060047 ,\n",
      "       0.99399525, 1.0060042 , 1.0060047 , 1.0060048 , 1.0060045 ,\n",
      "       1.0060014 , 1.0059998 , 1.0060047 , 0.9939949 , 0.99399495,\n",
      "       0.9939982 , 1.0060046 , 1.0060048 , 0.9939966 , 1.0060046 ,\n",
      "       0.993996  , 0.99399585, 1.0060045 , 0.99403924, 1.0060045 ,\n",
      "       1.0060045 , 0.9939951 , 0.9939949 , 1.0060042 , 1.0060042 ,\n",
      "       0.99399793, 0.99400413, 1.00599   , 1.006003  , 1.0060046 ,\n",
      "       0.993996  , 0.9939989 ], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600415, -0.00600455, -0.00600431,  0.00600516,  0.00600504,\n",
      "       -0.00600361,  0.00600469, -0.00600497,  0.0060045 , -0.00600376,\n",
      "       -0.006005  ,  0.00600505,  0.0060046 ,  0.00600498, -0.00600449,\n",
      "        0.00600448, -0.00600405,  0.00600468, -0.00600481,  0.0060041 ,\n",
      "        0.00600318, -0.0060048 ,  0.00600371, -0.00600477, -0.00600225,\n",
      "       -0.00600507, -0.00600525,  0.00600508,  0.00600516, -0.00600525,\n",
      "       -0.00600451, -0.0059817 ,  0.00600482,  0.00600197, -0.00600451,\n",
      "       -0.00600518, -0.00600515, -0.00600429,  0.00600521,  0.00600536,\n",
      "        0.0060053 , -0.00600535, -0.00600522,  0.00600531, -0.00600523,\n",
      "       -0.00600521,  0.00600389,  0.00600446,  0.00600462, -0.00598789,\n",
      "        0.00600456,  0.00600522,  0.00600284, -0.00600515, -0.00600511,\n",
      "       -0.00600472,  0.0059801 ,  0.00600407, -0.00600188, -0.006005  ,\n",
      "       -0.00600262, -0.00600431,  0.00600531,  0.00600013,  0.00600526,\n",
      "       -0.00600512,  0.00600509,  0.00600541, -0.00600519,  0.00600532,\n",
      "        0.00600493, -0.00600532,  0.00600475, -0.00600414,  0.00600533,\n",
      "       -0.00600515, -0.00600513, -0.00600085, -0.00600358, -0.00600258,\n",
      "        0.00600374,  0.00600383,  0.00600395,  0.00599787, -0.00600258,\n",
      "        0.00600542,  0.00600494,  0.00600459,  0.00600482, -0.00600256,\n",
      "        0.00600503,  0.00599753,  0.00600537, -0.00600506,  0.00600531,\n",
      "        0.00600527,  0.00600487, -0.00600518,  0.0060052 ,  0.00600531,\n",
      "       -0.00600509, -0.00600503, -0.00600506, -0.00600473,  0.00600531,\n",
      "        0.00600496, -0.00600346,  0.00600507, -0.00600534, -0.0060053 ,\n",
      "        0.00600404,  0.00600374, -0.00600209, -0.00599266,  0.00600516,\n",
      "        0.00600468,  0.00600403, -0.00600284,  0.00600436,  0.00600108,\n",
      "        0.00600535,  0.0060005 ,  0.00600353,  0.00600464, -0.00600484,\n",
      "        0.00600508,  0.00600508, -0.00600475, -0.00600492, -0.00600523,\n",
      "        0.00600532, -0.00600504, -0.0059885 ,  0.00600495,  0.00600517,\n",
      "       -0.00600523,  0.00600525, -0.00600522,  0.00600492,  0.0060053 ,\n",
      "       -0.006003  , -0.00600497, -0.00600245,  0.00600521,  0.00600374,\n",
      "       -0.00600534,  0.00600504,  0.00600507, -0.00599366,  0.00600433,\n",
      "       -0.00600499,  0.0060052 , -0.00600486,  0.00600467,  0.00600032,\n",
      "       -0.00600486, -0.00600477, -0.00600518, -0.00600527,  0.0060045 ,\n",
      "       -0.00600335, -0.00600517, -0.00600459, -0.0060041 , -0.00600471,\n",
      "       -0.00599977,  0.00600455, -0.00600511, -0.00600486, -0.00600518,\n",
      "       -0.00600525, -0.00600531, -0.0060045 , -0.00600534,  0.00600029,\n",
      "       -0.00600159,  0.00600524, -0.00600214,  0.00600518, -0.00600465,\n",
      "       -0.00600503, -0.00600502,  0.00600416,  0.00600524, -0.00600442,\n",
      "       -0.00600529,  0.0060005 , -0.00600526, -0.00599427,  0.0059935 ,\n",
      "       -0.00600319, -0.00600519,  0.00600508, -0.00600487,  0.00600486,\n",
      "        0.00600503,  0.00600372,  0.0060044 ,  0.00600523,  0.00600509,\n",
      "       -0.00600363,  0.00599866,  0.00600535,  0.00600483,  0.00600395,\n",
      "       -0.00600495, -0.00600488, -0.00600472,  0.00600502,  0.00599254,\n",
      "       -0.00600492, -0.00600534, -0.00600304, -0.00600527,  0.00600528,\n",
      "       -0.00600507, -0.00600516, -0.00600383, -0.00600455,  0.00600524,\n",
      "        0.00600512, -0.00600512, -0.00600376, -0.00599535,  0.00599084,\n",
      "        0.00600534,  0.00600516,  0.00600525, -0.00600387, -0.00600466,\n",
      "        0.00600516, -0.00600111, -0.00600444, -0.00600532,  0.00600508,\n",
      "       -0.00600484, -0.00299232, -0.00600371, -0.00600337,  0.0060053 ,\n",
      "        0.0060043 ,  0.00600531, -0.00600524, -0.00600516, -0.0060049 ,\n",
      "       -0.00600481, -0.00598944,  0.00600469,  0.00600497,  0.00600521,\n",
      "       -0.00600228,  0.00600525, -0.00600415, -0.00600507,  0.00600536,\n",
      "       -0.00600502, -0.00597598,  0.00600409, -0.00600487,  0.00600534,\n",
      "        0.0060049 ,  0.00600523,  0.0060041 , -0.00600518,  0.00600512,\n",
      "        0.0060051 ,  0.00600449, -0.00598943, -0.00600527,  0.00600013,\n",
      "        0.00600416, -0.00600506, -0.00600423, -0.0060051 , -0.0060036 ,\n",
      "        0.00600512,  0.00600439, -0.00600527,  0.00600436,  0.00600418,\n",
      "       -0.00600306,  0.00600503,  0.00600494, -0.00600507,  0.0060026 ,\n",
      "       -0.00600462, -0.00600376,  0.00600501,  0.00600451, -0.0060052 ,\n",
      "        0.00600534, -0.00600459,  0.00599233,  0.00600209,  0.00600483,\n",
      "       -0.00600504, -0.00600497,  0.00600528, -0.00600526, -0.00600336,\n",
      "       -0.00600487,  0.0059861 ,  0.00600523,  0.00600506, -0.00600496,\n",
      "       -0.00600516,  0.00600524,  0.00600533,  0.00600127, -0.00600525,\n",
      "        0.00600516, -0.00600488, -0.00600524, -0.006005  , -0.00600513,\n",
      "        0.00600484,  0.00600528,  0.0060053 , -0.00600504,  0.00600498,\n",
      "        0.00600367, -0.00600508,  0.00600515,  0.00600385,  0.00600525,\n",
      "       -0.00600525,  0.00600527,  0.00600503,  0.00600304,  0.00600506,\n",
      "       -0.00600496, -0.00600491,  0.00600469,  0.0060048 , -0.00599641,\n",
      "       -0.00600483,  0.0060052 , -0.00600511, -0.00600512,  0.00600308,\n",
      "       -0.00600511,  0.00599677,  0.00600489,  0.00600314,  0.00600391,\n",
      "       -0.00600518,  0.00600425, -0.00600494, -0.00600344,  0.006005  ,\n",
      "        0.00600525, -0.00600519,  0.00600516,  0.0060051 , -0.00600073,\n",
      "       -0.00600541,  0.00600513, -0.00599951, -0.00599474, -0.00600521,\n",
      "        0.00600441, -0.00600243,  0.00600523, -0.0060053 ,  0.00600477,\n",
      "        0.00600471, -0.00600479, -0.00600529, -0.00600505, -0.00600537,\n",
      "       -0.00600535,  0.00600503,  0.00600487,  0.00600345,  0.00600507,\n",
      "        0.00600441, -0.00600454,  0.00597463,  0.00600525, -0.00600432,\n",
      "        0.00600459, -0.00600493,  0.00600494, -0.00600528, -0.00600539,\n",
      "        0.00600315, -0.00600532, -0.00600474,  0.00600324, -0.00600541,\n",
      "        0.00600531,  0.00600513,  0.00600488, -0.00600489, -0.00600509,\n",
      "       -0.00600498, -0.006005  , -0.00600414, -0.00600487, -0.00600271,\n",
      "       -0.00600483, -0.0060048 ,  0.00600229, -0.00600497,  0.00600452,\n",
      "        0.00600473, -0.00600526,  0.0060053 ,  0.00600426,  0.00600506,\n",
      "        0.00600533,  0.00600517,  0.00600391,  0.0060023 ,  0.00600526,\n",
      "        0.00600533, -0.00600533, -0.00600502, -0.00600482,  0.00600542,\n",
      "       -0.00600396, -0.00600531,  0.00600528,  0.00594075,  0.00600512,\n",
      "       -0.00600342, -0.00600524,  0.00600457, -0.00600521, -0.0060053 ,\n",
      "        0.00588061,  0.00599195, -0.0060048 , -0.00600537,  0.00599893,\n",
      "       -0.0060049 ,  0.00600422, -0.00600472, -0.00600528,  0.0060052 ,\n",
      "       -0.00600291, -0.00600496,  0.00600525, -0.00600271, -0.0060051 ,\n",
      "        0.00600225, -0.00600284,  0.00600291, -0.00600498,  0.00600457,\n",
      "       -0.006005  , -0.00600239, -0.00600503,  0.00600528, -0.00600533,\n",
      "       -0.00600525,  0.00600529, -0.0060054 , -0.00600324,  0.00599266,\n",
      "       -0.00600491,  0.0060045 , -0.00599861, -0.00599915,  0.00600496,\n",
      "       -0.00600529, -0.00600501, -0.00600494, -0.00600507, -0.00600533,\n",
      "        0.00600394,  0.00600402, -0.00600523, -0.00600507, -0.00600481,\n",
      "       -0.00600537,  0.00600519,  0.00600094, -0.00600498,  0.00600514,\n",
      "       -0.00600492,  0.00600467,  0.00600507,  0.00600527,  0.00600486,\n",
      "        0.00600309,  0.00600482,  0.00600532, -0.00600519, -0.00600515,\n",
      "       -0.00600278,  0.00600484,  0.00600532, -0.00600427,  0.00600501,\n",
      "       -0.0060049 , -0.00600502,  0.00600468, -0.00600025,  0.00600429,\n",
      "        0.00600488, -0.00600498, -0.00600519,  0.00600471,  0.00600429,\n",
      "       -0.00600385, -0.00600337, -0.00600338,  0.006004  ,  0.00600519,\n",
      "       -0.0060043 , -0.00600279], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([1.0060034 , 0.9939969 , 0.9940151 , 1.0060004 , 0.9939951 ,\n",
      "       1.0060016 , 0.99399537, 1.005998  , 0.9939999 , 1.0060009 ,\n",
      "       1.0060029 , 1.0059631 , 1.0059984 , 0.99399495, 0.9939952 ,\n",
      "       1.0060042 , 1.006004  , 0.99399626, 1.0060034 , 0.9940946 ,\n",
      "       0.99399817, 1.0059993 , 1.0060004 , 0.9939966 , 0.9939957 ,\n",
      "       1.0060045 , 1.0060048 , 0.9939992 , 0.9939949 , 1.0060045 ,\n",
      "       0.99399585, 1.0059783 , 1.0059869 , 1.0059996 , 1.005997  ,\n",
      "       1.0060043 , 1.0059862 , 0.9940024 , 0.9939949 , 0.9939947 ,\n",
      "       1.0059882 , 1.0060045 , 1.0060045 , 1.0059968 , 0.9939957 ,\n",
      "       1.0060048 , 0.9939981 , 0.99399537, 0.9939966 , 0.9939974 ,\n",
      "       0.9939949 , 0.99400085, 0.99399495, 0.9940058 , 0.99402153,\n",
      "       1.0060042 , 0.9940012 , 1.0060034 , 1.0060045 , 1.0060047 ,\n",
      "       1.0059967 , 1.0059998 , 0.9939957 , 1.0060045 , 1.0060048 ,\n",
      "       1.0060033 , 0.9939979 , 1.0060045 , 1.0060029 , 1.0059927 ,\n",
      "       0.99401754, 1.0060004 , 0.9939957 , 1.0060022 , 0.99399483,\n",
      "       1.0060024 , 0.99399644, 0.9939993 , 1.0060042 , 0.99400324,\n",
      "       0.9939993 , 1.0060043 , 0.99399555, 0.99399614, 1.0060002 ,\n",
      "       1.0060049 , 0.9939962 , 0.99402356, 0.993995  , 0.99405307,\n",
      "       0.99399817, 0.9939996 , 1.0060048 , 0.9940162 , 0.9939965 ,\n",
      "       1.0060048 , 0.9940051 , 0.99399817, 1.0060043 , 1.0060049 ,\n",
      "       1.0060029 , 0.99400324, 0.9940002 , 0.99400336, 1.0060048 ,\n",
      "       1.0060045 , 0.993996  , 0.9939955 , 1.0060034 , 0.99399537,\n",
      "       1.0059972 , 1.0060029 , 0.9945983 , 0.9939975 , 1.0060047 ,\n",
      "       0.99399495, 0.9940136 , 1.0059886 , 1.0059923 , 0.99399716,\n",
      "       1.0060047 , 1.005976  , 1.0060034 , 1.0060042 , 1.0060028 ,\n",
      "       1.0060042 , 1.0060042 , 1.006004  , 0.9939951 , 0.99399555,\n",
      "       0.9939949 , 1.0060024 , 1.0060034 , 0.99404037, 0.9939956 ,\n",
      "       0.993996  , 1.0060042 , 1.0060034 , 1.0059993 , 0.9939949 ,\n",
      "       1.0060021 , 1.0059347 , 1.0060042 , 1.0060042 , 0.9940144 ,\n",
      "       1.0060046 , 0.99399817, 0.99399555, 0.99400955, 1.006004  ,\n",
      "       0.99399763, 0.9939951 , 1.005991  , 0.99399555, 1.0060028 ,\n",
      "       1.0060042 , 0.99399495, 1.0060045 , 1.0060041 , 0.9939963 ,\n",
      "       1.0058708 , 1.0060045 , 1.0059906 , 1.0060037 , 1.0060045 ,\n",
      "       0.99399817, 1.0060004 , 1.0060048 , 0.99399644, 0.9939951 ,\n",
      "       0.9939965 , 0.993995  , 1.0059825 , 0.99399495, 0.9940833 ,\n",
      "       0.9940023 , 0.99399537, 1.0060011 , 0.99399537, 1.0060002 ,\n",
      "       1.006004  , 1.0060048 , 0.9939973 , 0.9939963 , 1.0060027 ,\n",
      "       1.0060048 , 1.0060048 , 0.99399495, 0.9939953 , 1.0060021 ,\n",
      "       0.9939956 , 1.0060042 , 0.993995  , 1.0060025 , 0.99401736,\n",
      "       1.0059944 , 0.99400264, 0.99399644, 0.9939983 , 0.99399537,\n",
      "       1.0059693 , 0.9940273 , 1.0060014 , 1.0059706 , 0.9940031 ,\n",
      "       0.9940061 , 1.0059644 , 1.0060014 , 1.0060003 , 1.0060004 ,\n",
      "       0.99399954, 0.99399495, 1.0059948 , 0.9939961 , 0.993995  ,\n",
      "       1.0060034 , 1.0060027 , 1.0060045 , 0.9939976 , 0.993995  ,\n",
      "       1.0059972 , 1.0060042 , 1.0060045 , 1.0060016 , 1.0060029 ,\n",
      "       0.9939957 , 0.99399716, 1.0060037 , 1.0060034 , 1.0060045 ,\n",
      "       0.9939951 , 1.0060048 , 1.0059991 , 0.99399585, 0.9939949 ,\n",
      "       0.99399716, 1.0060004 , 1.0060023 , 1.0060045 , 0.9939949 ,\n",
      "       0.99400294, 0.9939946 , 0.9940023 , 1.0060048 , 1.0060028 ,\n",
      "       1.0060048 , 1.0059813 , 0.99399644, 1.0060046 , 1.0059319 ,\n",
      "       1.0059975 , 0.9939949 , 1.0060042 , 0.9939961 , 0.99399513,\n",
      "       0.99399495, 1.0060024 , 1.0060022 , 1.0060042 , 0.99399495,\n",
      "       0.993999  , 1.0059966 , 0.9939995 , 1.0060034 , 0.99399996,\n",
      "       1.0060037 , 0.99399674, 1.0059036 , 1.0060042 , 0.99400336,\n",
      "       1.0060042 , 1.0060047 , 1.0060002 , 1.0060014 , 1.0059996 ,\n",
      "       0.9939951 , 0.993998  , 1.0059974 , 1.0060029 , 0.99399763,\n",
      "       1.0060021 , 1.0060014 , 0.9939949 , 1.0059993 , 0.99399537,\n",
      "       1.0060023 , 0.99399585, 0.9939952 , 0.99399644, 1.0060048 ,\n",
      "       0.993995  , 1.0060045 , 1.0060037 , 0.994317  , 0.9939994 ,\n",
      "       1.0060042 , 1.0060028 , 0.99399644, 0.99399495, 0.9940069 ,\n",
      "       1.0060045 , 1.0059948 , 0.9939983 , 0.99399614, 1.0060004 ,\n",
      "       0.99399525, 0.99399495, 0.99399555, 1.0053995 , 0.9939961 ,\n",
      "       0.9939956 , 1.0060029 , 1.0060042 , 0.9940652 , 1.0060045 ,\n",
      "       0.9939957 , 0.99399585, 0.99399495, 0.9939965 , 0.99399495,\n",
      "       0.9939951 , 1.0060048 , 0.99399626, 0.99399936, 0.99399495,\n",
      "       1.0060025 , 0.99399495, 0.99399674, 0.99399763, 0.9939949 ,\n",
      "       1.0059904 , 1.0060023 , 1.0060042 , 0.9939961 , 0.9940058 ,\n",
      "       1.0060034 , 0.99399817, 1.0060029 , 0.99402803, 1.0060045 ,\n",
      "       1.0060049 , 1.0060024 , 0.99399495, 0.9939979 , 0.9939961 ,\n",
      "       1.0060048 , 0.99404323, 1.0060048 , 0.9939976 , 0.9939951 ,\n",
      "       0.99399483, 1.0060042 , 0.9939956 , 0.9939958 , 1.0060042 ,\n",
      "       1.0060034 , 0.9940099 , 0.9939981 , 0.99400395, 1.0060042 ,\n",
      "       0.99399537, 1.0059999 , 1.0060049 , 0.9939956 , 1.0060029 ,\n",
      "       1.0060043 , 0.99399644, 1.006004  , 1.0060012 , 0.9939947 ,\n",
      "       0.99399674, 1.0060023 , 0.99399644, 0.9939965 , 1.0060017 ,\n",
      "       1.0060029 , 0.9939957 , 1.0060024 , 1.0060045 , 1.0060042 ,\n",
      "       0.99401134, 0.99399537, 1.0060003 , 0.99399495, 0.993995  ,\n",
      "       1.005996  , 0.9939949 , 0.99399537, 0.9940017 , 0.9939949 ,\n",
      "       1.0060037 , 1.0059993 , 1.0060031 , 0.99399495, 0.9939955 ,\n",
      "       1.0060034 , 1.0059985 , 0.99400306, 1.0059974 , 1.0059999 ,\n",
      "       0.99399686, 1.0060028 , 0.99399537, 0.9939961 , 1.0060016 ,\n",
      "       0.99400234, 1.0060029 , 1.0060046 , 1.0060042 , 1.0060049 ,\n",
      "       1.0060048 , 1.0060047 , 0.9940095 , 1.0060048 , 1.0060045 ,\n",
      "       1.0060049 , 0.9940003 , 1.0060029 , 0.9939949 , 1.0060049 ,\n",
      "       1.006002  , 1.0060042 , 1.0060047 , 1.0060029 , 1.0060045 ,\n",
      "       0.9939957 , 0.9939947 , 1.0060045 , 0.9939951 , 0.99399483,\n",
      "       0.99399644, 0.99401146, 1.0060027 , 0.9939951 , 1.0059975 ,\n",
      "       1.0059906 , 1.0060045 , 0.99400586, 0.9939949 , 0.9939957 ,\n",
      "       0.9940081 , 0.99399513, 1.0060042 , 1.0060033 , 0.99399644,\n",
      "       0.9939979 , 0.9939957 , 1.0060045 , 0.9939992 , 1.00599   ,\n",
      "       0.9939949 , 1.0060042 , 0.9939949 , 1.0060042 , 0.9939947 ,\n",
      "       0.99399513, 1.0060048 , 0.99399495, 0.9940001 , 0.99399495,\n",
      "       0.9939958 , 1.0060021 , 1.0060045 , 1.0060031 , 1.0060047 ,\n",
      "       1.0059817 , 0.99399626, 1.0060042 , 0.99399555, 0.99399495,\n",
      "       0.99399644, 1.0060027 , 0.9939965 , 0.9939957 , 0.99399817,\n",
      "       0.9939949 , 1.0060042 , 1.0060045 , 0.9939957 , 1.0060048 ,\n",
      "       0.9939975 , 1.0059987 , 1.0060045 , 1.0060046 , 1.0060045 ,\n",
      "       1.006002  , 1.0060037 , 1.0060048 , 0.99399495, 0.99399495,\n",
      "       0.99402833, 1.0060048 , 1.0060045 , 0.9940002 , 1.0060037 ,\n",
      "       0.99399537, 0.99399716, 1.006004  , 1.0060045 , 0.99399614,\n",
      "       0.9939976 , 0.99399537, 1.0059723 , 1.0060048 , 1.0060042 ,\n",
      "       1.0060028 , 0.9939951 , 1.0060018 , 1.0059962 , 1.0060017 ,\n",
      "       1.0059999 , 0.9939963 ], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_0/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600455, -0.00600476,  0.00592337,  0.00600529,  0.00600507,\n",
      "       -0.00600344,  0.00600516, -0.00600447,  0.00600391,  0.00600271,\n",
      "       -0.00600501,  0.0060047 , -0.00597367,  0.00600517, -0.00600519,\n",
      "       -0.00600477, -0.00600443,  0.00600476, -0.00600489,  0.00600241,\n",
      "        0.00600481, -0.00599733,  0.00600488, -0.00599713, -0.00600482,\n",
      "       -0.00600477, -0.00600538,  0.0060051 ,  0.00600528, -0.00600532,\n",
      "       -0.00600454,  0.00600514, -0.00600479, -0.00600448,  0.00600446,\n",
      "       -0.00600502, -0.00600525,  0.00599886,  0.00600517,  0.00600536,\n",
      "        0.00600527, -0.00600539, -0.00600521,  0.0060051 , -0.00600522,\n",
      "       -0.00600526, -0.00600292,  0.0060051 ,  0.00600452, -0.0060049 ,\n",
      "        0.00600522,  0.00600501, -0.00600514, -0.0060049 , -0.00600523,\n",
      "       -0.00600457, -0.00600218, -0.00600457, -0.00600458, -0.00600502,\n",
      "        0.0060004 , -0.00600325,  0.00600529, -0.00600518,  0.00600531,\n",
      "       -0.00600436,  0.00600486,  0.00600544, -0.00600491,  0.00600537,\n",
      "        0.0060037 , -0.00600522, -0.00600487, -0.00600392,  0.00600536,\n",
      "       -0.0060038 , -0.00600519, -0.00600306, -0.00600499,  0.00600418,\n",
      "        0.00600227, -0.0060042 ,  0.00600513,  0.00600429,  0.00600449,\n",
      "        0.0060054 ,  0.00600497,  0.00600472,  0.00600504,  0.00600507,\n",
      "        0.00600434,  0.00600513,  0.00600537, -0.00600282,  0.00600536,\n",
      "        0.00600518,  0.00600435, -0.00600468,  0.00600473,  0.00600538,\n",
      "       -0.00600514,  0.00600256, -0.00600467, -0.0060038 ,  0.00600539,\n",
      "        0.00600517, -0.00600479,  0.00600526, -0.00600541, -0.00600537,\n",
      "        0.00600425,  0.00600514, -0.00599828,  0.00600449,  0.00600529,\n",
      "        0.00600508,  0.00600316,  0.00600493,  0.00599758,  0.00599163,\n",
      "        0.00600534, -0.00600405, -0.00600423,  0.00600495,  0.00600021,\n",
      "        0.00600518,  0.00600516, -0.00600496, -0.00600508, -0.00600528,\n",
      "        0.00600531, -0.00600494, -0.0060042 ,  0.0060045 ,  0.00600494,\n",
      "       -0.00600528,  0.00600527, -0.00600477,  0.00597375,  0.00600536,\n",
      "       -0.0060029 , -0.00600521, -0.00600458,  0.00600523, -0.00600454,\n",
      "       -0.00600518,  0.00600485,  0.00600524,  0.00600414, -0.00600347,\n",
      "       -0.00600522,  0.00600522, -0.00600459,  0.00600502, -0.00600197,\n",
      "       -0.00600512, -0.0060053 , -0.00600525, -0.00600514,  0.0060049 ,\n",
      "       -0.00600518, -0.00600524, -0.00600414, -0.0060036 , -0.00600509,\n",
      "       -0.00600101,  0.00600049, -0.00600532, -0.00600522, -0.00600529,\n",
      "       -0.00600528, -0.00600515, -0.00600453, -0.00600539, -0.00600527,\n",
      "       -0.00600366,  0.00600522, -0.00599021,  0.00600525, -0.0059849 ,\n",
      "       -0.0060047 , -0.00600532,  0.0060048 ,  0.0060053 , -0.00600373,\n",
      "       -0.00600532, -0.00600523, -0.00600522,  0.0060049 ,  0.00600478,\n",
      "        0.0060049 , -0.00600529,  0.00600519, -0.00600496,  0.00600472,\n",
      "        0.00600498,  0.00600239,  0.00600446,  0.00600533,  0.00600509,\n",
      "        0.00600291,  0.00587987,  0.00600538,  0.00600384,  0.00600507,\n",
      "        0.00600231,  0.00600144, -0.00600425,  0.00597924, -0.00600488,\n",
      "        0.00599997, -0.00600527,  0.00600393, -0.00600523,  0.00600535,\n",
      "       -0.00600501, -0.00600317, -0.00600509,  0.00599351,  0.00600527,\n",
      "        0.0060053 , -0.00600494, -0.00600471, -0.00600318,  0.00600449,\n",
      "        0.0060054 ,  0.00600378,  0.00600517, -0.00600455, -0.00600457,\n",
      "        0.00600512, -0.00600523, -0.00600084, -0.00600531,  0.00600515,\n",
      "       -0.006005  , -0.00599987, -0.00600504, -0.00600497,  0.00600531,\n",
      "       -0.00600262,  0.00600542, -0.00600534, -0.00600534, -0.00600509,\n",
      "       -0.00600528,  0.00599404,  0.00600508,  0.00600501,  0.00600491,\n",
      "        0.00600338,  0.00600531, -0.00600529, -0.00600531,  0.00600526,\n",
      "       -0.00600526,  0.00600379, -0.00600495, -0.00600431,  0.00600524,\n",
      "        0.00600378,  0.00599488,  0.00600309, -0.00600514,  0.00600526,\n",
      "        0.00600496,  0.00600414, -0.00600283, -0.00600514, -0.00598401,\n",
      "       -0.00600464, -0.00600507, -0.00600377, -0.00600473, -0.00599593,\n",
      "        0.00600532, -0.00600457, -0.00600519,  0.00600189,  0.00600516,\n",
      "        0.00600445, -0.00600213,  0.0060053 , -0.00600474,  0.00600499,\n",
      "       -0.00600215, -0.00595883,  0.00600524,  0.00600443, -0.00600522,\n",
      "        0.00600537, -0.00600516, -0.00600372, -0.00599789,  0.00600511,\n",
      "       -0.00600532, -0.00600505,  0.00600417, -0.00600522,  0.00600314,\n",
      "       -0.00600528, -0.00600448,  0.00600526,  0.00600498, -0.00600516,\n",
      "       -0.00600511,  0.0060053 ,  0.00600535,  0.00600474, -0.00600519,\n",
      "        0.00600522, -0.00600514, -0.00600508, -0.00600504, -0.00600521,\n",
      "        0.00600503,  0.00600483,  0.00600505, -0.00600515,  0.00600501,\n",
      "        0.00600474, -0.00600518,  0.00600494,  0.00600315,  0.00600532,\n",
      "       -0.00600539,  0.00600525,  0.00600512,  0.00600442,  0.00600527,\n",
      "        0.00597915, -0.00600005,  0.0060053 ,  0.0060045 , -0.00600471,\n",
      "       -0.00600514,  0.00600475, -0.00600427, -0.00600502, -0.00600478,\n",
      "       -0.00600513, -0.00600362,  0.00600502,  0.00600434,  0.006005  ,\n",
      "       -0.00600525,  0.00600489, -0.00600521, -0.00588553,  0.006005  ,\n",
      "        0.00600525, -0.00600516,  0.00600518,  0.00600516, -0.00600529,\n",
      "       -0.00600543,  0.00600358,  0.00600484,  0.00600472, -0.00600461,\n",
      "        0.00600432,  0.00600501,  0.00600539, -0.00600509,  0.00600477,\n",
      "        0.00600517, -0.00600491, -0.00600527, -0.00600198, -0.00600535,\n",
      "       -0.00600525,  0.00600455,  0.00600519, -0.00600404,  0.00600425,\n",
      "        0.00600518, -0.00600478,  0.0060042 ,  0.00600509,  0.00600487,\n",
      "       -0.00600306, -0.00600506,  0.00600398, -0.00600527, -0.00600538,\n",
      "       -0.00600004, -0.00600537, -0.00600515, -0.00600307, -0.00600542,\n",
      "        0.00600534,  0.00599787,  0.00600445, -0.00600503, -0.0060051 ,\n",
      "        0.00600395, -0.00600471, -0.00600432, -0.006003  ,  0.00600472,\n",
      "       -0.00600379, -0.00600382, -0.00600451, -0.00600473,  0.00600497,\n",
      "       -0.00600458, -0.00600491,  0.00600529,  0.00600376,  0.00600522,\n",
      "        0.00600529,  0.006005  ,  0.00599729,  0.00600503,  0.00600514,\n",
      "        0.00600534, -0.00600531,  0.00600361, -0.00600525,  0.00600544,\n",
      "        0.00600475, -0.00600538,  0.00600527,  0.00600425,  0.00600491,\n",
      "       -0.00600496, -0.00600532,  0.00600504, -0.0060053 , -0.00600534,\n",
      "       -0.00600479,  0.00596736,  0.00600299, -0.00600529,  0.00600483,\n",
      "        0.0060009 ,  0.0060047 , -0.00600465, -0.00600525,  0.0060051 ,\n",
      "       -0.0059853 , -0.00600496,  0.00600491,  0.00600468, -0.00600456,\n",
      "       -0.00600452, -0.00600443,  0.00600512, -0.00600426,  0.00600461,\n",
      "       -0.00600514,  0.00600475, -0.0060053 ,  0.00600511, -0.00600533,\n",
      "       -0.00600516,  0.00600527, -0.00600538, -0.00600255, -0.00600513,\n",
      "       -0.00600503,  0.00600522,  0.00600443,  0.00600471,  0.00600519,\n",
      "       -0.00600533, -0.006004  ,  0.00600273, -0.00600478, -0.00600533,\n",
      "       -0.00600466,  0.00600503, -0.00600528, -0.00600503, -0.00599692,\n",
      "       -0.0060054 ,  0.00600481,  0.00600498, -0.00600518,  0.00600508,\n",
      "       -0.00600495,  0.00600159,  0.00600527,  0.0060052 ,  0.0060051 ,\n",
      "        0.00600427,  0.0060051 ,  0.00600538, -0.00600506, -0.00600523,\n",
      "       -0.00600243,  0.00600519,  0.00600534, -0.006005  ,  0.00600498,\n",
      "       -0.00600491, -0.0060046 ,  0.00599866,  0.00600498, -0.0060035 ,\n",
      "        0.00600451, -0.00600506, -0.00600458,  0.00600532,  0.00600445,\n",
      "        0.00600303, -0.00600515,  0.00599937,  0.00599826,  0.00600472,\n",
      "        0.00600363, -0.00600456], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/query/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[ 4.57895733e-03,  3.12232063e-04, -6.61421195e-03, ...,\n",
      "          3.63030797e-03,  1.99926808e-03, -2.17286497e-03],\n",
      "        [-1.42844766e-03,  7.92625267e-03,  4.74111596e-03, ...,\n",
      "          3.73657560e-03,  1.25471770e-03,  4.30507213e-03],\n",
      "        [ 4.44736844e-03,  5.61298989e-03, -3.63736856e-03, ...,\n",
      "         -3.89257469e-03, -6.84379274e-03,  3.84991732e-03]],\n",
      "\n",
      "       [[-5.31217549e-03, -4.80306149e-03,  8.80318985e-04, ...,\n",
      "          1.35729485e-03, -6.14555040e-03, -1.09709823e-03],\n",
      "        [ 7.40662124e-03, -5.60497108e-04, -5.41543588e-03, ...,\n",
      "         -7.63502065e-03,  9.16576071e-04, -1.52254663e-03],\n",
      "        [-2.58448673e-03, -6.35239761e-03,  1.05494335e-04, ...,\n",
      "          7.00619246e-04,  2.12674751e-03, -6.05646009e-03]],\n",
      "\n",
      "       [[ 2.05280539e-03,  2.46894429e-03, -2.82640639e-03, ...,\n",
      "          4.59908368e-03,  7.76341278e-03, -1.53717678e-03],\n",
      "        [-2.90059694e-03,  5.63509064e-03,  6.28463505e-03, ...,\n",
      "          2.49131094e-03,  6.88939402e-03,  2.16857600e-03],\n",
      "        [ 2.35757022e-03, -1.04414660e-03, -5.54486876e-03, ...,\n",
      "         -7.50782574e-03,  7.61662435e-04,  2.41156365e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-5.15660644e-03,  7.96921668e-04,  1.39327382e-03, ...,\n",
      "         -5.77208912e-03, -4.08317102e-03, -3.10694007e-03],\n",
      "        [ 9.07599402e-04,  1.14954752e-03, -5.41322632e-03, ...,\n",
      "         -6.00497564e-03, -1.59474348e-05, -5.25697321e-03],\n",
      "        [-4.35131183e-03, -3.78167955e-03,  5.30302757e-03, ...,\n",
      "          5.60121657e-03, -1.32723397e-03, -6.08767942e-03]],\n",
      "\n",
      "       [[-1.46719732e-03,  2.68050679e-03,  6.28685998e-03, ...,\n",
      "         -5.63887693e-03, -7.42396479e-03,  4.36498784e-03],\n",
      "        [ 5.62075991e-03, -3.23545141e-03, -4.57958644e-03, ...,\n",
      "         -4.19437513e-03,  1.61068211e-03, -7.60552939e-03],\n",
      "        [-4.89212992e-03, -1.96496816e-03,  3.89160751e-03, ...,\n",
      "         -1.27696991e-03,  1.45729631e-04, -7.20659795e-04]],\n",
      "\n",
      "       [[-8.63339473e-03, -1.65277917e-04,  5.72826201e-03, ...,\n",
      "         -6.74846582e-03, -2.01910571e-03,  4.52401303e-03],\n",
      "        [-8.85374553e-04, -5.19466121e-03, -7.51831150e-03, ...,\n",
      "         -5.09303482e-03, -7.61414412e-03, -3.42641980e-03],\n",
      "        [-8.47880822e-03, -8.37973226e-03,  4.98027122e-03, ...,\n",
      "          7.00080348e-03,  5.51821152e-03, -7.28613464e-04]]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/query/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[-0.00491614, -0.00354667,  0.00436022, ..., -0.00471423,\n",
      "        -0.00536873, -0.00051064],\n",
      "       [ 0.00455584, -0.00514052, -0.00499153, ..., -0.00560419,\n",
      "        -0.00455481, -0.0049635 ],\n",
      "       [-0.0050556 , -0.00452508,  0.00515297, ...,  0.00469132,\n",
      "         0.00522274, -0.00527618]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/key/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[-4.0924065e-03,  8.3345693e-04, -2.6263327e-03, ...,\n",
      "          2.7702581e-03,  2.7201893e-03,  1.3192892e-03],\n",
      "        [ 8.9363789e-04,  2.3099573e-03, -2.5606276e-03, ...,\n",
      "          6.7858933e-04, -6.3619320e-04, -2.1744968e-04],\n",
      "        [ 7.0029171e-05,  4.2812322e-04,  1.9607216e-03, ...,\n",
      "          6.3967751e-03, -1.3345736e-03, -1.2502039e-03]],\n",
      "\n",
      "       [[ 4.3571633e-03, -2.3720895e-03, -6.2520583e-03, ...,\n",
      "         -4.6654306e-03,  3.3800399e-03,  5.4836129e-03],\n",
      "        [-4.2337137e-03, -1.4872390e-03,  6.3017686e-03, ...,\n",
      "         -2.1269417e-03, -6.1499729e-04, -1.3323750e-03],\n",
      "        [ 6.9271615e-03,  8.9206137e-03,  7.0141661e-03, ...,\n",
      "          7.9222154e-03, -5.6274291e-03,  1.9514238e-03]],\n",
      "\n",
      "       [[-3.4796741e-04,  1.2467549e-03, -1.4942889e-03, ...,\n",
      "         -3.1223847e-03, -9.3106524e-04,  1.9766798e-03],\n",
      "        [ 3.0931202e-03, -3.0725596e-03,  1.6330403e-03, ...,\n",
      "          3.4908317e-03, -3.6395036e-03, -2.0250306e-03],\n",
      "        [-4.8619076e-03, -3.7846514e-03,  4.5651832e-04, ...,\n",
      "         -1.1313291e-03,  7.5032969e-04, -4.8185447e-03]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-6.3905152e-03,  6.1832606e-03,  4.5913095e-03, ...,\n",
      "         -1.2005254e-03,  1.9961006e-03, -6.8382323e-03],\n",
      "        [ 6.3529219e-03, -1.3676872e-03,  7.9807444e-03, ...,\n",
      "          2.3139925e-03,  3.1684847e-03, -1.2023853e-03],\n",
      "        [ 6.2494352e-04, -3.7505603e-04,  5.7959203e-03, ...,\n",
      "          3.3076641e-03, -5.2821739e-03,  1.8107392e-03]],\n",
      "\n",
      "       [[-4.8095114e-03,  2.4116782e-03,  6.8308529e-03, ...,\n",
      "          5.9166818e-04, -5.9581781e-03, -2.2187848e-03],\n",
      "        [ 2.0708996e-03, -5.9844949e-03,  3.6732184e-03, ...,\n",
      "          2.4873125e-03,  3.1174097e-04, -7.2846880e-05],\n",
      "        [ 3.3810975e-03,  1.1568565e-03, -6.6595513e-04, ...,\n",
      "          2.6867280e-03, -1.1365120e-03,  1.4642157e-03]],\n",
      "\n",
      "       [[-4.4481987e-03, -1.9854460e-04,  6.6403542e-03, ...,\n",
      "         -3.9101727e-03, -1.0566309e-03, -6.2574712e-03],\n",
      "        [ 4.5648529e-05,  8.0521051e-03, -7.7964882e-03, ...,\n",
      "         -2.2678540e-05,  1.4068129e-03, -2.9270158e-03],\n",
      "        [-5.5732667e-03,  2.1452457e-03,  2.7211236e-03, ...,\n",
      "         -4.3436382e-03,  1.0331083e-04,  2.3522268e-03]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/key/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[ 1.2783050e-07,  8.2246480e-08,  8.8697092e-08, ...,\n",
      "        -1.6180710e-08,  1.7717660e-07, -1.8072429e-07],\n",
      "       [ 5.0638306e-08, -3.2468829e-08, -1.5478825e-07, ...,\n",
      "         1.4417189e-07, -3.1858363e-07,  1.3600119e-07],\n",
      "       [-6.1994179e-08, -2.2161699e-07, -2.2774474e-07, ...,\n",
      "        -1.5911547e-07,  2.7887640e-07, -4.1204167e-08]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/value/kernel:0' shape=(512, 3, 512) dtype=float32, numpy=\n",
      "array([[[ 0.00460718,  0.00273601,  0.00393701, ...,  0.00282836,\n",
      "         -0.00307002,  0.00390928],\n",
      "        [-0.00782987,  0.0048667 , -0.0091645 , ...,  0.00259134,\n",
      "          0.00265812,  0.00828946],\n",
      "        [ 0.00565461, -0.00596274,  0.00565573, ..., -0.00169052,\n",
      "          0.00615638, -0.00382864]],\n",
      "\n",
      "       [[-0.00182795, -0.00124297, -0.00776296, ..., -0.00475011,\n",
      "          0.00284717, -0.0050758 ],\n",
      "        [ 0.00749653, -0.0035805 ,  0.0103524 , ..., -0.00218801,\n",
      "         -0.01054311, -0.00606556],\n",
      "        [-0.00847769,  0.00132952, -0.00542642, ...,  0.00151279,\n",
      "         -0.0073724 ,  0.00861748]],\n",
      "\n",
      "       [[ 0.00221589,  0.00329326,  0.00849033, ...,  0.00960413,\n",
      "         -0.00215369,  0.00651519],\n",
      "        [-0.00950632,  0.00721245, -0.00459365, ...,  0.00655503,\n",
      "          0.00248059,  0.00421317],\n",
      "        [ 0.00573459, -0.00475937,  0.00904159, ..., -0.00726762,\n",
      "          0.00383098, -0.0066995 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.00780915, -0.0077524 , -0.00440768, ..., -0.00488709,\n",
      "          0.00283546, -0.01034478],\n",
      "        [ 0.00307195, -0.00754738,  0.006589  , ..., -0.00676967,\n",
      "         -0.0054425 , -0.00158204],\n",
      "        [-0.00287843,  0.0101062 , -0.010137  , ...,  0.00344113,\n",
      "         -0.00227567,  0.00381657]],\n",
      "\n",
      "       [[-0.01006156, -0.00297483, -0.00526923, ..., -0.0080495 ,\n",
      "          0.005581  , -0.00958043],\n",
      "        [ 0.00260519, -0.00792844,  0.0065388 , ..., -0.00781852,\n",
      "         -0.00652833, -0.00964867],\n",
      "        [-0.00871731,  0.01075029, -0.00968872, ...,  0.00347904,\n",
      "         -0.008178  ,  0.01039251]],\n",
      "\n",
      "       [[-0.0015424 , -0.0031848 , -0.00146129, ..., -0.0044696 ,\n",
      "          0.00740755, -0.00668596],\n",
      "        [ 0.00437875, -0.00867247,  0.00900902, ..., -0.00430341,\n",
      "         -0.00154228, -0.00363742],\n",
      "        [-0.00890311,  0.0013631 , -0.00272696, ...,  0.00759737,\n",
      "         -0.00163529,  0.0052297 ]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/value/bias:0' shape=(3, 512) dtype=float32, numpy=\n",
      "array([[-0.00600451, -0.00600468, -0.00600397, ..., -0.00600513,\n",
      "         0.00600503, -0.00600374],\n",
      "       [ 0.00599956, -0.00599484,  0.00600514, ..., -0.00600519,\n",
      "        -0.00600501, -0.00600463],\n",
      "       [-0.00600444,  0.00600501, -0.0060051 , ...,  0.00600511,\n",
      "        -0.0060051 ,  0.00599902]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/attention_output/kernel:0' shape=(3, 512, 512) dtype=float32, numpy=\n",
      "array([[[ 0.04642331,  0.0100567 ,  0.02922092, ...,  0.00850972,\n",
      "         -0.00229872,  0.0307873 ],\n",
      "        [-0.02279496,  0.03206464,  0.04392082, ..., -0.00619527,\n",
      "          0.0397429 , -0.02496023],\n",
      "        [-0.0246022 ,  0.00030062, -0.0009153 , ..., -0.01734773,\n",
      "         -0.03353969, -0.02381527],\n",
      "        ...,\n",
      "        [-0.0149995 , -0.0047464 ,  0.02680161, ...,  0.03389341,\n",
      "         -0.0228175 ,  0.00943364],\n",
      "        [ 0.01895369, -0.03335328, -0.02127633, ..., -0.01542995,\n",
      "          0.01005616, -0.02786458],\n",
      "        [ 0.02426822, -0.04796973, -0.03453987, ...,  0.02819841,\n",
      "          0.0235407 , -0.0144987 ]],\n",
      "\n",
      "       [[-0.04605278,  0.00079673,  0.02581783, ...,  0.00517098,\n",
      "         -0.01106391, -0.01454743],\n",
      "        [-0.02088517,  0.01020159,  0.03228628, ..., -0.03093   ,\n",
      "         -0.00199731, -0.00610665],\n",
      "        [-0.04475413,  0.01104957,  0.03620812, ..., -0.03208398,\n",
      "          0.0200038 ,  0.03212101],\n",
      "        ...,\n",
      "        [ 0.04601467, -0.01672313, -0.01343709, ..., -0.04836629,\n",
      "         -0.04624978,  0.00861819],\n",
      "        [ 0.03258217, -0.03728363,  0.04106453, ..., -0.00207741,\n",
      "         -0.01098274,  0.00358153],\n",
      "        [-0.00396477, -0.01697535,  0.00803353, ...,  0.00774488,\n",
      "          0.01060738,  0.00521224]],\n",
      "\n",
      "       [[ 0.02264266,  0.03722604,  0.03435026, ...,  0.03202725,\n",
      "         -0.04189519,  0.02244551],\n",
      "        [-0.04512765, -0.03086153,  0.02742517, ..., -0.00873435,\n",
      "          0.0017896 , -0.00035521],\n",
      "        [ 0.04224442,  0.00563795, -0.04914368, ...,  0.03244739,\n",
      "         -0.00153735, -0.0326306 ],\n",
      "        ...,\n",
      "        [ 0.01912417,  0.00629491,  0.00830088, ..., -0.03947149,\n",
      "          0.0099622 ,  0.04594884],\n",
      "        [-0.0074954 , -0.04771949, -0.01323529, ...,  0.00164307,\n",
      "          0.02873144,  0.0099862 ],\n",
      "        [-0.02439996,  0.02646611, -0.0114835 , ...,  0.02206577,\n",
      "          0.03179542,  0.03208389]]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/multi_head_attention_1/attention_output/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600462, -0.0060048 ,  0.00594736,  0.00600529,  0.00600509,\n",
      "       -0.00600225,  0.00600519, -0.00600457,  0.00600328, -0.00582047,\n",
      "       -0.00600503,  0.00600457,  0.00598201,  0.00600517, -0.0060052 ,\n",
      "       -0.00600477, -0.00600454,  0.00600479, -0.00600485,  0.00600115,\n",
      "        0.00600479, -0.00600254,  0.00600474, -0.0059939 , -0.00600495,\n",
      "       -0.0060047 , -0.00600538,  0.0060051 ,  0.00600529, -0.00600531,\n",
      "       -0.00600416,  0.00600508, -0.00600483, -0.00600468,  0.00600417,\n",
      "       -0.00600508, -0.0060052 , -0.00542354,  0.00600518,  0.00600536,\n",
      "        0.00600528, -0.00600539, -0.00600521,  0.0060051 , -0.00600522,\n",
      "       -0.00600527, -0.006     ,  0.0060051 ,  0.00600441, -0.00600494,\n",
      "        0.00600522,  0.00600501, -0.0060051 , -0.00600482, -0.00600523,\n",
      "       -0.00600408, -0.00599753, -0.00600457, -0.00600445, -0.00600497,\n",
      "        0.00600274, -0.00600367,  0.00600529, -0.00600512,  0.00600531,\n",
      "       -0.00600449,  0.006005  ,  0.00600544, -0.00600495,  0.00600536,\n",
      "        0.00600457, -0.00600523, -0.00600479, -0.00600425,  0.00600537,\n",
      "       -0.00600302, -0.00600523, -0.0060031 , -0.00600501,  0.00600412,\n",
      "        0.00600036, -0.00600363,  0.00600513,  0.00600429,  0.00600452,\n",
      "        0.00600541,  0.00600498,  0.00600465,  0.00600506,  0.00600506,\n",
      "        0.00600461,  0.00600513,  0.00600537, -0.0059988 ,  0.00600536,\n",
      "        0.00600515,  0.00600426, -0.00600472,  0.00600471,  0.00600539,\n",
      "       -0.00600516,  0.0060003 , -0.0060046 , -0.00600314,  0.00600539,\n",
      "        0.0060051 , -0.0060047 ,  0.00600525, -0.00600541, -0.00600537,\n",
      "        0.0060036 ,  0.00600513, -0.00599236,  0.00600431,  0.00600527,\n",
      "        0.00600513,  0.00600091,  0.00600494,  0.00600227,  0.00600033,\n",
      "        0.00600534, -0.00600429, -0.00600385,  0.00600491,  0.00600259,\n",
      "        0.00600518,  0.00600513, -0.0060049 , -0.00600508, -0.0060053 ,\n",
      "        0.00600532, -0.00600498, -0.00600425,  0.00600455,  0.00600496,\n",
      "       -0.00600527,  0.00600527, -0.00600486,  0.00600098,  0.00600535,\n",
      "       -0.00600279, -0.00600523, -0.00600466,  0.00600522, -0.00600475,\n",
      "       -0.00600518,  0.00600485,  0.0060052 ,  0.00600424, -0.0060022 ,\n",
      "       -0.00600521,  0.00600521, -0.00600486,  0.00600501, -0.00599934,\n",
      "       -0.00600509, -0.00600531, -0.00600523, -0.00600509,  0.00600479,\n",
      "       -0.00600516, -0.00600521, -0.00600424, -0.00600365, -0.00600506,\n",
      "       -0.00599507,  0.00600307, -0.00600531, -0.00600521, -0.00600528,\n",
      "       -0.00600528, -0.00600519, -0.00600456, -0.00600539, -0.00600526,\n",
      "       -0.00600423,  0.00600522, -0.00599885,  0.00600525, -0.00593827,\n",
      "       -0.0060045 , -0.00600531,  0.00600475,  0.0060053 , -0.00600262,\n",
      "       -0.00600533, -0.00600524, -0.00600525,  0.00600494,  0.00600472,\n",
      "        0.00600485, -0.00600529,  0.00600519, -0.00600501,  0.00600466,\n",
      "        0.00600499,  0.00600265,  0.00600448,  0.00600533,  0.00600511,\n",
      "        0.00600248,  0.00597548,  0.00600538,  0.00600438,  0.00600505,\n",
      "        0.00600271,  0.00600195, -0.00600408,  0.00599908, -0.00600491,\n",
      "        0.00599842, -0.00600527,  0.00600368, -0.00600524,  0.00600534,\n",
      "       -0.00600493, -0.00600339, -0.00600509, -0.00600239,  0.00600528,\n",
      "        0.00600531, -0.00600489, -0.00600472, -0.00600046,  0.00600471,\n",
      "        0.0060054 ,  0.00600332,  0.00600516, -0.00600475, -0.00600446,\n",
      "        0.0060051 , -0.0060052 , -0.00600202, -0.0060053 ,  0.00600505,\n",
      "       -0.00600492, -0.00599955, -0.00600503, -0.00600498,  0.00600531,\n",
      "       -0.00600254,  0.00600542, -0.00600534, -0.00600534, -0.00600509,\n",
      "       -0.00600525,  0.00600217,  0.00600509,  0.006005  ,  0.00600495,\n",
      "        0.00600359,  0.00600532, -0.00600528, -0.00600532,  0.00600528,\n",
      "       -0.00600526,  0.00600431, -0.00600492, -0.00600421,  0.00600523,\n",
      "        0.00600394,  0.00600279,  0.0059851 , -0.00600517,  0.00600526,\n",
      "        0.00600494,  0.0060033 , -0.0060027 , -0.00600521,  0.00599461,\n",
      "       -0.0060047 , -0.00600507, -0.00600357, -0.00600487, -0.00599433,\n",
      "        0.00600532, -0.00600463, -0.0060052 ,  0.00600336,  0.00600517,\n",
      "        0.00600456, -0.00600376,  0.00600529, -0.00600466,  0.00600496,\n",
      "       -0.00600362,  0.0060011 ,  0.00600521,  0.00600422, -0.00600519,\n",
      "        0.00600536, -0.00600518, -0.00600366, -0.00599214,  0.00600512,\n",
      "       -0.00600532, -0.006005  ,  0.00600426, -0.00600526,  0.00600186,\n",
      "       -0.00600528, -0.00600449,  0.00600526,  0.00600481, -0.00600516,\n",
      "       -0.00600513,  0.00600528,  0.00600534,  0.00600481, -0.00600518,\n",
      "        0.0060052 , -0.00600516, -0.0060051 , -0.00600501, -0.00600519,\n",
      "        0.00600499,  0.00600478,  0.00600509, -0.00600519,  0.00600506,\n",
      "        0.00600482, -0.00600518,  0.00600495,  0.00600315,  0.00600533,\n",
      "       -0.00600539,  0.00600524,  0.00600515,  0.00600422,  0.00600525,\n",
      "       -0.00593152, -0.00600205,  0.00600529,  0.00600448, -0.00600425,\n",
      "       -0.00600517,  0.00600468, -0.00600405, -0.00600495, -0.00600487,\n",
      "       -0.00600512, -0.00600405,  0.00600501,  0.00600452,  0.00600501,\n",
      "       -0.00600525,  0.00600485, -0.0060052 ,  0.00599964,  0.00600494,\n",
      "        0.00600524, -0.00600519,  0.00600515,  0.00600517, -0.0060053 ,\n",
      "       -0.00600543,  0.00600406,  0.00600475,  0.00600457, -0.0060045 ,\n",
      "        0.00600398,  0.00600498,  0.00600539, -0.0060051 ,  0.00600486,\n",
      "        0.00600515, -0.00600495, -0.00600527, -0.0060036 , -0.00600535,\n",
      "       -0.00600523,  0.00600443,  0.00600518, -0.00600429,  0.0060044 ,\n",
      "        0.00600517, -0.00600482,  0.00600394,  0.00600508,  0.00600491,\n",
      "       -0.00599961, -0.0060051 ,  0.00600333, -0.00600528, -0.00600538,\n",
      "       -0.00600123, -0.00600538, -0.00600514, -0.00600328, -0.00600541,\n",
      "        0.00600534, -0.00596525,  0.00600436, -0.00600507, -0.00600513,\n",
      "        0.00600178, -0.00600476, -0.00600389, -0.00600027,  0.00600473,\n",
      "       -0.00600414, -0.00600401, -0.00600438, -0.00600437,  0.00600498,\n",
      "       -0.00600458, -0.0060048 ,  0.00600529,  0.00600298,  0.00600521,\n",
      "        0.00600528,  0.00600499,  0.00599973,  0.00600501,  0.00600518,\n",
      "        0.00600534, -0.00600532,  0.00600377, -0.00600526,  0.00600544,\n",
      "        0.00600482, -0.00600538,  0.00600529,  0.0060036 ,  0.00600493,\n",
      "       -0.00600491, -0.00600532,  0.00600499, -0.0060053 , -0.00600532,\n",
      "       -0.00600484, -0.00599462,  0.00600306, -0.00600526,  0.00600483,\n",
      "        0.00599965,  0.00600488, -0.00600476, -0.00600527,  0.00600512,\n",
      "        0.00596858, -0.00600496,  0.00600479,  0.00600474, -0.00600449,\n",
      "       -0.00600466, -0.00600363,  0.00600512, -0.00600397,  0.00600397,\n",
      "       -0.00600514,  0.00600424, -0.00600531,  0.00600514, -0.00600532,\n",
      "       -0.00600516,  0.00600527, -0.00600538, -0.0060038 , -0.00600514,\n",
      "       -0.00600505,  0.00600525,  0.00600441,  0.00600478,  0.00600515,\n",
      "       -0.00600533, -0.00600305,  0.0060037 , -0.00600478, -0.00600534,\n",
      "       -0.0060046 ,  0.00600496, -0.00600524, -0.00600496,  0.00589619,\n",
      "       -0.00600541,  0.00600487,  0.006005  , -0.00600517,  0.00600512,\n",
      "       -0.00600499,  0.00599966,  0.00600526,  0.00600518,  0.00600507,\n",
      "        0.00600447,  0.00600516,  0.00600538, -0.00600511, -0.00600525,\n",
      "       -0.00600232,  0.00600517,  0.00600533, -0.006005  ,  0.00600492,\n",
      "       -0.00600487, -0.00600463,  0.00599159,  0.00600498, -0.00600427,\n",
      "        0.00600473, -0.00600504, -0.00600466,  0.00600533,  0.00600404,\n",
      "        0.00600269, -0.00600519,  0.00600321,  0.00595248,  0.00600455,\n",
      "        0.0060042 , -0.00600461], dtype=float32)>\n",
      "<tf.Variable 'dense_2/kernel:0' shape=(512, 1028) dtype=float32, numpy=\n",
      "array([[ 0.05267629,  0.00130293,  0.05716418, ..., -0.0028683 ,\n",
      "         0.02165551,  0.01470932],\n",
      "       [-0.01047502,  0.00088922, -0.06075126, ..., -0.03180296,\n",
      "         0.01922451,  0.02581209],\n",
      "       [ 0.052826  ,  0.0467387 ,  0.00787688, ...,  0.05098619,\n",
      "         0.04025131, -0.00552559],\n",
      "       ...,\n",
      "       [ 0.01998992, -0.00458289, -0.02715985, ..., -0.0269517 ,\n",
      "        -0.04809145,  0.03150242],\n",
      "       [-0.03056148, -0.05111743, -0.02423736, ...,  0.0397907 ,\n",
      "        -0.00728183,  0.001162  ],\n",
      "       [-0.03934713, -0.02399092, -0.0025204 , ..., -0.03530503,\n",
      "         0.0164829 ,  0.06200274]], dtype=float32)>\n",
      "<tf.Variable 'dense_2/bias:0' shape=(1028,) dtype=float32, numpy=\n",
      "array([ 0.00599984,  0.00600016, -0.00600321, ...,  0.00600454,\n",
      "       -0.00600482,  0.00600173], dtype=float32)>\n",
      "<tf.Variable 'dense_3/kernel:0' shape=(1028, 512) dtype=float32, numpy=\n",
      "array([[-0.03620987,  0.02978847,  0.01813841, ...,  0.06369323,\n",
      "        -0.04720898,  0.05950595],\n",
      "       [ 0.00415412, -0.0298507 ,  0.04491688, ...,  0.03351608,\n",
      "        -0.04876407, -0.00436411],\n",
      "       [-0.02919757, -0.04950932, -0.00600654, ..., -0.02169495,\n",
      "        -0.05012927, -0.00071636],\n",
      "       ...,\n",
      "       [ 0.05022661,  0.02623239,  0.01734258, ...,  0.00232539,\n",
      "         0.05988874, -0.02574181],\n",
      "       [ 0.04500938,  0.04611472, -0.00415303, ..., -0.02889645,\n",
      "         0.02637232,  0.01650207],\n",
      "       [-0.00327502,  0.05214685, -0.0114172 , ...,  0.05890461,\n",
      "         0.06747772, -0.05122676]], dtype=float32)>\n",
      "<tf.Variable 'dense_3/bias:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600518, -0.00599377, -0.00598159,  0.00600523,  0.00600454,\n",
      "        0.00600321,  0.00600528, -0.00600515, -0.00600441, -0.0060047 ,\n",
      "       -0.0060048 ,  0.00600446,  0.00600464,  0.00600511, -0.00600525,\n",
      "       -0.00600397,  0.00600406,  0.00599212, -0.00600412,  0.00600406,\n",
      "       -0.00600442, -0.00600457,  0.00600492, -0.00599843,  0.00600324,\n",
      "       -0.00600285, -0.00600533,  0.00600472,  0.00600525, -0.00600517,\n",
      "       -0.00600494,  0.00600521, -0.00600514, -0.00600055, -0.00600149,\n",
      "       -0.0060046 , -0.00600527, -0.00599979,  0.00600502,  0.00600525,\n",
      "        0.00600507, -0.0060053 , -0.0060053 ,  0.00600523, -0.00600526,\n",
      "       -0.00600524,  0.00600461,  0.00600485, -0.00600336, -0.00600444,\n",
      "        0.00600503,  0.00600498, -0.00600481, -0.00600496, -0.00600536,\n",
      "       -0.0060048 , -0.00600501, -0.00560028,  0.00600283, -0.00600517,\n",
      "        0.00600483, -0.00600453,  0.0060053 , -0.00600404,  0.00600517,\n",
      "       -0.0060049 ,  0.00600388,  0.00600529, -0.00600525,  0.00600533,\n",
      "        0.00600493, -0.00600529,  0.00600334, -0.00600498,  0.0060052 ,\n",
      "        0.00600216, -0.00600447,  0.00597286, -0.00600509,  0.00600436,\n",
      "       -0.00600498,  0.00600496,  0.00600512, -0.0059722 ,  0.00600498,\n",
      "        0.00600528,  0.00600528,  0.00600506,  0.00600523,  0.00600493,\n",
      "       -0.00600225,  0.00600452,  0.00600535,  0.00600179,  0.00600523,\n",
      "        0.00600513, -0.00600454, -0.00600383,  0.00600492,  0.00600355,\n",
      "       -0.00600527,  0.00600482, -0.00600517, -0.00600506,  0.00600538,\n",
      "        0.00600504, -0.00600459,  0.00600521, -0.00600537, -0.00600518,\n",
      "        0.00600497,  0.00600485,  0.00568588,  0.00600484,  0.00600507,\n",
      "        0.00599501, -0.00600462,  0.00600489, -0.00600414, -0.00596231,\n",
      "        0.00600512, -0.00600419, -0.00600411,  0.00600511, -0.006005  ,\n",
      "        0.0060053 ,  0.00600515, -0.00600489,  0.00600239, -0.00600534,\n",
      "        0.00600521,  0.00600477,  0.00599891,  0.00600433,  0.00600464,\n",
      "       -0.00600534,  0.00600513, -0.00600427,  0.00600449,  0.00600515,\n",
      "       -0.00600377, -0.00600518, -0.0060038 ,  0.00600526, -0.00600479,\n",
      "       -0.00600361,  0.0060045 ,  0.00600519,  0.00600453, -0.00600304,\n",
      "       -0.00600507,  0.00600522,  0.00600205,  0.00600508, -0.0060045 ,\n",
      "       -0.00600525, -0.00600521, -0.0060052 , -0.00600506,  0.0060052 ,\n",
      "       -0.00600521, -0.00600531, -0.00600519, -0.00600461, -0.00600497,\n",
      "       -0.0060008 , -0.00600409, -0.00600527, -0.00600518, -0.00600529,\n",
      "       -0.00600454, -0.00600511, -0.00600374, -0.00600523, -0.00600531,\n",
      "       -0.00600372,  0.0060043 , -0.00600478,  0.00600519,  0.00600483,\n",
      "       -0.00600507, -0.00600532, -0.0060031 ,  0.00600527, -0.00600472,\n",
      "       -0.00600532, -0.00600526, -0.00600531,  0.00600502,  0.00599792,\n",
      "        0.00600405, -0.00600533,  0.00600527, -0.0060045 ,  0.00598813,\n",
      "        0.00600433, -0.00600376,  0.00600424,  0.00600512,  0.00600493,\n",
      "        0.00600352, -0.00600426,  0.00600537, -0.00600408,  0.00600504,\n",
      "       -0.00600162, -0.00600434,  0.00599668, -0.00600157, -0.00600508,\n",
      "        0.00599541, -0.00600527,  0.00600339, -0.00600536,  0.00600526,\n",
      "       -0.00600379,  0.0059972 , -0.00600404, -0.00600428,  0.00600522,\n",
      "        0.00600524,  0.00600131, -0.00600513,  0.00600496, -0.00600054,\n",
      "        0.0060053 ,  0.00600404,  0.00600442,  0.00600468, -0.00600508,\n",
      "        0.00600526, -0.00600524,  0.00600393, -0.00600525,  0.00600525,\n",
      "        0.00599755, -0.00600354, -0.00600497,  0.0059755 ,  0.00600527,\n",
      "       -0.00600507,  0.00600538, -0.00600532, -0.00600535, -0.00600493,\n",
      "       -0.00600397, -0.00600404,  0.00600466,  0.0060049 ,  0.00600494,\n",
      "        0.00600437,  0.00600536, -0.00600487, -0.00600526,  0.00600526,\n",
      "       -0.00600504,  0.00600463, -0.00600493, -0.0060035 ,  0.00600499,\n",
      "        0.00600511,  0.00600359,  0.00600447, -0.00600513,  0.00600518,\n",
      "        0.00600314,  0.00600442,  0.00600447, -0.00600483, -0.00600317,\n",
      "       -0.00600263, -0.00600515, -0.00600503,  0.00600482,  0.00600427,\n",
      "        0.00600519, -0.00287487, -0.00600522,  0.00599944,  0.00600493,\n",
      "        0.00600419,  0.00593471,  0.00600477, -0.00600096,  0.00600506,\n",
      "        0.00600331,  0.00600496,  0.00600527,  0.00599552, -0.00600526,\n",
      "        0.00600538, -0.00599888,  0.00600338, -0.00600457,  0.00600504,\n",
      "       -0.00600509,  0.00599864,  0.0059996 , -0.0060053 , -0.0060018 ,\n",
      "       -0.00600519, -0.00600485,  0.00600516,  0.00600497, -0.00600453,\n",
      "        0.00600189,  0.00600526,  0.0060053 ,  0.00599872, -0.00600523,\n",
      "        0.00600515, -0.00600452, -0.00600493, -0.00600443, -0.00600521,\n",
      "        0.00600529,  0.00600504,  0.00600519, -0.00600515,  0.00600529,\n",
      "        0.00600516, -0.00600526,  0.00600517, -0.00600406,  0.00600519,\n",
      "       -0.00600538,  0.0060051 ,  0.00600507, -0.0060042 ,  0.00600517,\n",
      "       -0.0059846 ,  0.006004  ,  0.00600531,  0.00600511, -0.00600499,\n",
      "       -0.00600517,  0.00600478, -0.00600502, -0.00600494, -0.00600527,\n",
      "       -0.00600503, -0.00600511,  0.00600484, -0.00600299,  0.00600507,\n",
      "       -0.00600523,  0.00600473, -0.00600464, -0.00600521,  0.00600458,\n",
      "        0.00600521, -0.00600518,  0.00600512,  0.00600529, -0.00600521,\n",
      "       -0.00600539,  0.00600497,  0.00600467,  0.00600444, -0.00595545,\n",
      "        0.00600506,  0.00600421,  0.00600528, -0.00600506,  0.00600401,\n",
      "        0.00600517, -0.00600471, -0.00600532, -0.00600481, -0.00600524,\n",
      "       -0.00600511,  0.00600489,  0.00600526, -0.0060046 , -0.00598901,\n",
      "        0.00600463, -0.00600313,  0.00600518,  0.00600505,  0.00600511,\n",
      "        0.00600322,  0.00600442, -0.00599504, -0.00600512, -0.00600534,\n",
      "       -0.00599869, -0.00600531,  0.0060014 , -0.00600479, -0.00600537,\n",
      "        0.00600523,  0.00600432,  0.00600466, -0.00600496, -0.00600512,\n",
      "        0.00600523, -0.00600516,  0.00600404,  0.00600512,  0.00600513,\n",
      "       -0.00600519,  0.00600428, -0.00600494,  0.00600438,  0.00598218,\n",
      "        0.00600274, -0.00597515,  0.00600524, -0.00600451,  0.00600529,\n",
      "        0.00600508,  0.00600512,  0.00600438,  0.00600512,  0.00600477,\n",
      "        0.00600536, -0.00600525,  0.00600437, -0.00600513,  0.00600538,\n",
      "        0.0060049 , -0.0060054 ,  0.00600517,  0.00600496,  0.00600496,\n",
      "       -0.00600509, -0.00600518,  0.00600509, -0.00600511, -0.00600524,\n",
      "       -0.00600359,  0.00600344, -0.00600464, -0.00600521,  0.00600433,\n",
      "        0.00600496,  0.00600427, -0.00600489, -0.00600518,  0.00600511,\n",
      "       -0.00600515, -0.00600439,  0.00600322,  0.00600415,  0.00600313,\n",
      "       -0.00600027,  0.00600504,  0.00600474,  0.0060048 , -0.00600488,\n",
      "       -0.00600393, -0.00597289, -0.00600536,  0.00600485, -0.00600534,\n",
      "       -0.00600436,  0.00600465, -0.00600534, -0.00600486, -0.0060048 ,\n",
      "        0.00600306,  0.00600515,  0.0060045 ,  0.00600496,  0.00600531,\n",
      "       -0.00600528, -0.0060049 ,  0.0060045 , -0.00600517, -0.00600529,\n",
      "       -0.00600481,  0.00600464, -0.00600522, -0.00600521, -0.00600457,\n",
      "       -0.00600537,  0.0060016 , -0.00595925, -0.00600531,  0.00600519,\n",
      "       -0.00600258, -0.00600494,  0.00600529,  0.00600523,  0.00600259,\n",
      "        0.00600443,  0.00600521,  0.00600537, -0.00600503, -0.00600516,\n",
      "       -0.00600267,  0.00600512,  0.00600501, -0.00600123,  0.00600519,\n",
      "       -0.00600251, -0.0060052 ,  0.00599298,  0.00600508, -0.00600487,\n",
      "        0.00600458,  0.0059889 , -0.00599979,  0.00600523,  0.00600418,\n",
      "       -0.00600227, -0.00600526,  0.00600482,  0.00600446,  0.00600425,\n",
      "        0.00599702,  0.0060046 ], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/layer_normalization_2/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([1.0060033 , 0.9939969 , 0.9940149 , 1.0059896 , 0.99399513,\n",
      "       1.0060014 , 0.99399537, 1.0059996 , 0.99400145, 1.0060016 ,\n",
      "       1.0060024 , 0.99401146, 1.0059979 , 0.99399495, 0.9939951 ,\n",
      "       1.006004  , 1.0060042 , 0.99399614, 1.0060034 , 1.0059501 ,\n",
      "       0.99399877, 1.006002  , 1.0059993 , 0.9939966 , 0.99399537,\n",
      "       1.0060045 , 1.0060048 , 0.993998  , 0.9939949 , 1.0060043 ,\n",
      "       0.9939961 , 0.99404156, 1.0059472 , 1.0060003 , 1.0059968 ,\n",
      "       1.0060045 , 0.994027  , 0.994009  , 0.9939949 , 0.9939947 ,\n",
      "       1.0060012 , 1.0060045 , 1.0060045 , 1.0059954 , 0.99399585,\n",
      "       1.0060048 , 0.99399793, 0.99399537, 0.99399686, 0.99399674,\n",
      "       0.9939949 , 0.99400115, 0.99399495, 0.9940234 , 1.0059986 ,\n",
      "       1.0060037 , 0.9939996 , 1.0060033 , 1.0060045 , 1.0060046 ,\n",
      "       1.0059818 , 1.0060014 , 0.9939956 , 1.0060045 , 1.0060048 ,\n",
      "       1.0060034 , 0.99399745, 1.0060045 , 1.0060028 , 0.9940119 ,\n",
      "       0.994     , 1.0060008 , 0.9939958 , 1.0060024 , 0.9939948 ,\n",
      "       1.0060016 , 0.99399644, 0.993999  , 1.0060042 , 0.994006  ,\n",
      "       0.99399966, 1.0060042 , 0.99399555, 0.9939962 , 1.0060004 ,\n",
      "       1.0060049 , 0.99399626, 1.005763  , 0.99399495, 1.00597   ,\n",
      "       0.9939982 , 0.9939993 , 1.0060048 , 0.99541277, 0.993999  ,\n",
      "       1.0060048 , 0.9940192 , 0.99399817, 1.0060043 , 1.0060049 ,\n",
      "       1.0060028 , 0.9940011 , 0.9940011 , 0.99400336, 1.0060048 ,\n",
      "       1.0060042 , 0.9939961 , 0.9939956 , 1.0060034 , 0.99399513,\n",
      "       0.9940099 , 1.0060029 , 0.99409896, 0.99399793, 1.0060047 ,\n",
      "       0.9939949 , 0.9940296 , 1.0059906 , 1.0059998 , 0.9939972 ,\n",
      "       1.0060047 , 1.0059531 , 1.0060033 , 1.0060042 , 1.0060028 ,\n",
      "       1.0060043 , 1.0060042 , 1.0060034 , 0.9939951 , 0.99399537,\n",
      "       0.9939949 , 1.0060025 , 1.0060039 , 1.0059854 , 0.99399555,\n",
      "       0.99399585, 1.0060043 , 1.0060034 , 1.0059977 , 0.9939949 ,\n",
      "       1.0060024 , 1.0059993 , 1.0060042 , 1.0060042 , 0.99400586,\n",
      "       1.0060046 , 0.9939975 , 0.9939957 , 0.9940058 , 1.0060034 ,\n",
      "       0.99399745, 0.9939951 , 1.0059812 , 0.9939956 , 1.0060028 ,\n",
      "       1.0060042 , 0.99399495, 1.0060045 , 1.0060034 , 0.993997  ,\n",
      "       1.0059896 , 1.0060045 , 1.0059947 , 1.0060042 , 1.0060045 ,\n",
      "       0.9939976 , 1.0059998 , 1.0060048 , 0.9939962 , 0.993995  ,\n",
      "       0.993998  , 0.99399495, 1.00599   , 0.99399495, 0.99406177,\n",
      "       0.99399924, 0.99399537, 1.0060017 , 0.99399555, 1.0060002 ,\n",
      "       1.0060031 , 1.0060047 , 0.9939976 , 0.9939962 , 1.006002  ,\n",
      "       1.0060048 , 1.0060047 , 0.99399495, 0.99399513, 1.0060016 ,\n",
      "       0.9939957 , 1.0060042 , 0.99399495, 1.0060014 , 0.9940481 ,\n",
      "       1.0059938 , 0.99400264, 0.9939963 , 0.9939972 , 0.9939953 ,\n",
      "       1.0059761 , 0.99402326, 1.0060008 , 0.99400556, 0.9940018 ,\n",
      "       0.994007  , 1.0059245 , 1.0060011 , 1.0059987 , 1.0060002 ,\n",
      "       0.9940017 , 0.99399495, 1.0059962 , 0.99399585, 0.993995  ,\n",
      "       1.0060034 , 1.0060028 , 1.0060045 , 0.9939989 , 0.99399495,\n",
      "       1.0060029 , 1.006004  , 1.0060045 , 1.0059999 , 1.0060034 ,\n",
      "       0.9939971 , 0.9939989 , 1.0060042 , 1.0060042 , 1.0060043 ,\n",
      "       0.99399513, 1.0060048 , 1.0060003 , 0.99399626, 0.99399495,\n",
      "       0.9939983 , 1.0060004 , 1.0060014 , 1.0060045 , 0.9939949 ,\n",
      "       0.99400306, 0.9939946 , 1.0059644 , 1.0060047 , 1.0060024 ,\n",
      "       1.0060048 , 1.005959  , 0.9939971 , 1.0060046 , 1.005933  ,\n",
      "       1.0059993 , 0.9939949 , 1.0060042 , 0.9939965 , 0.993995  ,\n",
      "       0.99399495, 1.0060014 , 1.0060029 , 1.0060042 , 0.9939951 ,\n",
      "       0.99399817, 1.0059826 , 1.0059992 , 1.0060029 , 0.9940008 ,\n",
      "       1.0060033 , 0.9940031 , 0.9942217 , 1.0060043 , 0.9939995 ,\n",
      "       1.0060043 , 1.0060048 , 1.0059996 , 1.0060021 , 1.0059998 ,\n",
      "       0.9939951 , 0.99399716, 1.0059896 , 1.0060028 , 0.9939979 ,\n",
      "       1.0060018 , 1.0060029 , 0.9939949 , 1.006001  , 0.9939953 ,\n",
      "       1.0060029 , 0.9939957 , 0.99399537, 0.9939965 , 1.0060048 ,\n",
      "       0.9939953 , 1.0060045 , 1.0060034 , 1.0059301 , 0.993998  ,\n",
      "       1.0060042 , 1.0060024 , 0.9939962 , 0.99399495, 0.9940099 ,\n",
      "       1.0060045 , 1.0059967 , 0.9939971 , 0.99399716, 1.0059966 ,\n",
      "       0.9939951 , 0.99399495, 0.99399585, 1.0059859 , 0.99399644,\n",
      "       0.99399537, 1.006003  , 1.0060042 , 1.0059847 , 1.0060045 ,\n",
      "       0.99399585, 0.9939961 , 0.99399495, 0.993996  , 0.99399495,\n",
      "       0.99399495, 1.0060048 , 0.99399614, 0.9939996 , 0.993995  ,\n",
      "       1.0060014 , 0.9939951 , 0.99399644, 0.9939983 , 0.9939949 ,\n",
      "       1.0059927 , 1.0060028 , 1.0060042 , 0.9939961 , 0.994003  ,\n",
      "       1.006004  , 0.9939983 , 1.0060028 , 0.994002  , 1.0060045 ,\n",
      "       1.0060049 , 1.0060023 , 0.99399495, 0.9939972 , 0.9939962 ,\n",
      "       1.0060048 , 1.0059717 , 1.0060048 , 0.993997  , 0.9939953 ,\n",
      "       0.99399483, 1.0060042 , 0.99399585, 0.9939956 , 1.0060042 ,\n",
      "       1.0060042 , 0.99400336, 0.9940003 , 0.9940049 , 1.0060042 ,\n",
      "       0.99399555, 1.0059999 , 1.0060049 , 0.9939955 , 1.0060029 ,\n",
      "       1.0060042 , 0.9939965 , 1.0060042 , 1.0060022 , 0.9939947 ,\n",
      "       0.99399716, 1.0060021 , 0.9939966 , 0.9939965 , 1.0060014 ,\n",
      "       1.0060028 , 0.99399567, 1.0060023 , 1.0060045 , 1.0060042 ,\n",
      "       1.0057236 , 0.99399537, 1.0060014 , 0.99399495, 0.99399495,\n",
      "       1.0059944 , 0.9939949 , 0.99399537, 0.9940001 , 0.9939949 ,\n",
      "       1.006004  , 0.994058  , 1.0060029 , 0.99399495, 0.99399537,\n",
      "       1.0060029 , 1.0059993 , 0.99400455, 1.006     , 1.0060003 ,\n",
      "       0.99399626, 1.0060027 , 0.99399555, 0.99399644, 1.0060003 ,\n",
      "       0.9939995 , 1.0060028 , 1.0060046 , 1.0060033 , 1.0060048 ,\n",
      "       1.0060048 , 1.0060047 , 0.9940218 , 1.0060048 , 1.0060045 ,\n",
      "       1.0060049 , 0.99400324, 1.0060029 , 0.99399483, 1.0060049 ,\n",
      "       1.0060006 , 1.0060043 , 1.0060047 , 1.006002  , 1.0060045 ,\n",
      "       0.9939958 , 0.9939947 , 1.0060045 , 0.993995  , 0.9939949 ,\n",
      "       0.9939961 , 0.99400157, 1.0060027 , 0.99399525, 1.0059985 ,\n",
      "       1.0059514 , 1.0060046 , 0.9940011 , 0.99399483, 0.9939956 ,\n",
      "       0.9945617 , 0.99399513, 1.0060042 , 1.0060034 , 0.9939965 ,\n",
      "       0.9939979 , 0.99399644, 1.0060045 , 0.99400216, 1.0059906 ,\n",
      "       0.99399495, 1.0060033 , 0.9939949 , 1.0060042 , 0.9939947 ,\n",
      "       0.99399513, 1.0060048 , 0.99399495, 0.9939966 , 0.99399495,\n",
      "       0.9939958 , 1.0060024 , 1.0060045 , 1.0060034 , 1.0060046 ,\n",
      "       1.0059764 , 0.9939971 , 1.0060045 , 0.9939955 , 0.9939949 ,\n",
      "       0.99399644, 1.0060021 , 0.9939971 , 0.99399585, 0.99399817,\n",
      "       0.9939949 , 1.0060043 , 1.0060046 , 0.9939957 , 1.0060048 ,\n",
      "       0.9939971 , 1.0059872 , 1.0060045 , 1.0060045 , 1.0060045 ,\n",
      "       1.0060021 , 1.0060042 , 1.0060048 , 0.9939949 , 0.99399495,\n",
      "       0.9940214 , 1.0060048 , 1.0060045 , 0.99400306, 1.0060037 ,\n",
      "       0.9939956 , 0.9939966 , 1.0060033 , 1.0060045 , 0.99399567,\n",
      "       0.9939976 , 0.9939956 , 1.0059861 , 1.0060048 , 1.0060029 ,\n",
      "       1.0060025 , 0.993995  , 1.0060014 , 0.99407274, 1.006001  ,\n",
      "       1.0060012 , 0.99399614], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/layer_normalization_2/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600464, -0.00600481, -0.0045964 ,  0.00600529,  0.00600509,\n",
      "       -0.00600243,  0.00600519, -0.00600459,  0.00600319, -0.00596204,\n",
      "       -0.00600503,  0.00600455,  0.00596514,  0.00600517, -0.0060052 ,\n",
      "       -0.00600478, -0.00600456,  0.00600478, -0.00600486,  0.00600079,\n",
      "        0.00600478, -0.00600269,  0.00600473, -0.00599593, -0.00600495,\n",
      "       -0.00600472, -0.00600539,  0.00600509,  0.00600529, -0.00600531,\n",
      "       -0.0060042 ,  0.00600507, -0.00600483, -0.00600469,  0.00600414,\n",
      "       -0.00600508, -0.0060052 , -0.00595449,  0.00600518,  0.00600536,\n",
      "        0.00600528, -0.00600539, -0.00600521,  0.0060051 , -0.00600522,\n",
      "       -0.00600527, -0.0060005 ,  0.0060051 ,  0.00600439, -0.00600495,\n",
      "        0.00600522,  0.00600501, -0.00600511, -0.00600483, -0.00600523,\n",
      "       -0.00600412, -0.00599852, -0.00600458, -0.00600447, -0.00600497,\n",
      "        0.0060026 , -0.00600374,  0.00600529, -0.00600512,  0.00600531,\n",
      "       -0.00600451,  0.00600499,  0.00600543, -0.00600495,  0.00600536,\n",
      "        0.00600456, -0.00600523, -0.0060048 , -0.00600428,  0.00600537,\n",
      "       -0.00600313, -0.00600523, -0.0060032 , -0.00600502,  0.00600408,\n",
      "        0.00599984, -0.0060037 ,  0.00600513,  0.00600426,  0.0060045 ,\n",
      "        0.00600541,  0.00600498,  0.00600464,  0.00600505,  0.00600506,\n",
      "        0.00600459,  0.00600513,  0.00600537, -0.00599951,  0.00600536,\n",
      "        0.00600515,  0.00600423, -0.00600473,  0.0060047 ,  0.00600539,\n",
      "       -0.00600516,  0.00599977, -0.00600462, -0.00600324,  0.00600539,\n",
      "        0.0060051 , -0.00600471,  0.00600525, -0.00600541, -0.00600537,\n",
      "        0.00600353,  0.00600512, -0.00599483,  0.00600428,  0.00600527,\n",
      "        0.00600513,  0.00600049,  0.00600494,  0.00600207,  0.0059998 ,\n",
      "        0.00600534, -0.00600431, -0.0060039 ,  0.00600491,  0.00600243,\n",
      "        0.00600518,  0.00600512, -0.00600491, -0.00600509, -0.0060053 ,\n",
      "        0.00600532, -0.00600498, -0.00600428,  0.00600454,  0.00600495,\n",
      "       -0.00600528,  0.00600527, -0.00600486,  0.00600059,  0.00600535,\n",
      "       -0.00600292, -0.00600523, -0.00600467,  0.00600522, -0.00600476,\n",
      "       -0.00600518,  0.00600484,  0.0060052 ,  0.00600421, -0.0060024 ,\n",
      "       -0.00600521,  0.00600521, -0.00600486,  0.006005  , -0.00599995,\n",
      "       -0.00600509, -0.00600531, -0.00600523, -0.00600509,  0.00600478,\n",
      "       -0.00600516, -0.00600521, -0.00600427, -0.00600372, -0.00600507,\n",
      "       -0.00599668,  0.00600296, -0.00600532, -0.00600521, -0.00600528,\n",
      "       -0.00600528, -0.00600519, -0.00600458, -0.00600539, -0.00600526,\n",
      "       -0.00600426,  0.00600522, -0.00599955,  0.00600525, -0.00597462,\n",
      "       -0.00600451, -0.00600531,  0.00600474,  0.00600529, -0.00600276,\n",
      "       -0.00600533, -0.00600524, -0.00600525,  0.00600493,  0.00600471,\n",
      "        0.00600484, -0.00600529,  0.00600519, -0.00600501,  0.00600465,\n",
      "        0.00600499,  0.0060025 ,  0.00600446,  0.00600533,  0.00600511,\n",
      "        0.00600231,  0.00594163,  0.00600538,  0.00600435,  0.00600504,\n",
      "        0.00600257,  0.0060017 , -0.00600411,  0.00599826, -0.00600492,\n",
      "        0.0059974 , -0.00600527,  0.00600361, -0.00600524,  0.00600534,\n",
      "       -0.00600494, -0.00600347, -0.00600509, -0.00600255,  0.00600527,\n",
      "        0.00600531, -0.0060049 , -0.00600473, -0.00600088,  0.0060047 ,\n",
      "        0.0060054 ,  0.00600323,  0.00600515, -0.00600477, -0.00600448,\n",
      "        0.0060051 , -0.0060052 , -0.00600222, -0.0060053 ,  0.00600505,\n",
      "       -0.00600493, -0.00600013, -0.00600503, -0.00600499,  0.00600531,\n",
      "       -0.00600269,  0.00600542, -0.00600534, -0.00600534, -0.00600509,\n",
      "       -0.00600526,  0.00600195,  0.00600509,  0.00600499,  0.00600495,\n",
      "        0.00600352,  0.00600532, -0.00600529, -0.00600532,  0.00600527,\n",
      "       -0.00600527,  0.00600428, -0.00600492, -0.00600424,  0.00600523,\n",
      "        0.00600389,  0.00600264,  0.00597329, -0.00600517,  0.00600525,\n",
      "        0.00600493,  0.00600321, -0.00600284, -0.00600521,  0.005992  ,\n",
      "       -0.00600471, -0.00600508, -0.00600363, -0.00600488, -0.00599618,\n",
      "        0.00600532, -0.00600465, -0.00600521,  0.00600327,  0.00600516,\n",
      "        0.00600455, -0.00600381,  0.00600529, -0.00600468,  0.00600496,\n",
      "       -0.00600368,  0.00600074,  0.00600521,  0.00600419, -0.00600519,\n",
      "        0.00600536, -0.00600518, -0.00600372, -0.00599469,  0.00600512,\n",
      "       -0.00600532, -0.00600501,  0.00600423, -0.00600526,  0.0060016 ,\n",
      "       -0.00600528, -0.00600451,  0.00600526,  0.0060048 , -0.00600516,\n",
      "       -0.00600513,  0.00600527,  0.00600535,  0.0060048 , -0.00600518,\n",
      "        0.00600519, -0.00600516, -0.0060051 , -0.00600501, -0.0060052 ,\n",
      "        0.00600499,  0.00600477,  0.00600508, -0.00600519,  0.00600505,\n",
      "        0.00600481, -0.00600518,  0.00600495,  0.00600305,  0.00600533,\n",
      "       -0.00600539,  0.00600524,  0.00600515,  0.00600418,  0.00600525,\n",
      "       -0.00597339, -0.00600225,  0.00600529,  0.00600446, -0.00600428,\n",
      "       -0.00600517,  0.00600467, -0.00600409, -0.00600496, -0.00600488,\n",
      "       -0.00600512, -0.00600409,  0.006005  ,  0.0060045 ,  0.006005  ,\n",
      "       -0.00600526,  0.00600484, -0.0060052 ,  0.00599895,  0.00600493,\n",
      "        0.00600524, -0.00600519,  0.00600515,  0.00600516, -0.0060053 ,\n",
      "       -0.00600543,  0.00600401,  0.00600474,  0.00600456, -0.00600451,\n",
      "        0.00600393,  0.00600498,  0.00600539, -0.0060051 ,  0.00600486,\n",
      "        0.00600515, -0.00600496, -0.00600527, -0.00600367, -0.00600535,\n",
      "       -0.00600523,  0.00600441,  0.00600518, -0.00600432,  0.00600438,\n",
      "        0.00600517, -0.00600483,  0.00600389,  0.00600508,  0.0060049 ,\n",
      "       -0.00600017, -0.00600511,  0.00600324, -0.00600528, -0.00600538,\n",
      "       -0.00600154, -0.00600538, -0.00600515, -0.00600337, -0.00600541,\n",
      "        0.00600534, -0.00598184,  0.00600433, -0.00600507, -0.00600513,\n",
      "        0.00600151, -0.00600477, -0.00600393, -0.00600071,  0.00600472,\n",
      "       -0.00600417, -0.00600405, -0.0060044 , -0.00600439,  0.00600497,\n",
      "       -0.0060046 , -0.00600481,  0.00600529,  0.00600286,  0.00600521,\n",
      "        0.00600528,  0.00600498,  0.00599906,  0.006005  ,  0.00600518,\n",
      "        0.00600534, -0.00600532,  0.00600372, -0.00600526,  0.00600544,\n",
      "        0.00600481, -0.00600538,  0.00600529,  0.00600353,  0.00600493,\n",
      "       -0.00600492, -0.00600532,  0.00600499, -0.0060053 , -0.00600532,\n",
      "       -0.00600485, -0.00599638,  0.00600295, -0.00600526,  0.00600482,\n",
      "        0.00599896,  0.00600488, -0.00600478, -0.00600527,  0.00600512,\n",
      "        0.00589976, -0.00600497,  0.00600478,  0.00600473, -0.00600451,\n",
      "       -0.00600468, -0.00600369,  0.00600512, -0.00600402,  0.00600393,\n",
      "       -0.00600515,  0.0060042 , -0.00600531,  0.00600513, -0.00600532,\n",
      "       -0.00600516,  0.00600527, -0.00600538, -0.00600385, -0.00600514,\n",
      "       -0.00600506,  0.00600525,  0.00600438,  0.00600477,  0.00600514,\n",
      "       -0.00600533, -0.00600315,  0.00600364, -0.00600479, -0.00600534,\n",
      "       -0.00600461,  0.00600496, -0.00600524, -0.00600496, -0.00589301,\n",
      "       -0.00600541,  0.00600486,  0.006005  , -0.00600517,  0.00600512,\n",
      "       -0.006005  ,  0.00599898,  0.00600526,  0.00600517,  0.00600506,\n",
      "        0.00600445,  0.00600516,  0.00600538, -0.00600511, -0.00600525,\n",
      "       -0.00600249,  0.00600517,  0.00600533, -0.006005  ,  0.00600492,\n",
      "       -0.00600488, -0.00600464,  0.005987  ,  0.00600497, -0.0060043 ,\n",
      "        0.00600472, -0.00600504, -0.00600468,  0.00600532,  0.006004  ,\n",
      "        0.00600254, -0.0060052 ,  0.00600311,  0.00519191,  0.00600454,\n",
      "        0.00600417, -0.00600463], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/layer_normalization_3/gamma:0' shape=(512,) dtype=float32, numpy=\n",
      "array([0.99399585, 0.99400157, 0.99403983, 1.0060034 , 0.9939956 ,\n",
      "       0.99399644, 0.9939951 , 1.0060033 , 1.0060029 , 0.9939958 ,\n",
      "       1.0060031 , 0.99399585, 1.0060014 , 0.99399495, 1.0060043 ,\n",
      "       1.0060025 , 0.99399614, 0.9939994 , 0.9940006 , 0.9939982 ,\n",
      "       0.9940439 , 1.0060023 , 0.99399954, 0.99399555, 1.0060042 ,\n",
      "       1.0060008 , 1.0060048 , 0.99400157, 0.9939947 , 1.0060042 ,\n",
      "       0.9940126 , 1.0060045 , 1.0060004 , 1.0059978 , 1.0060028 ,\n",
      "       1.0060031 , 1.0060048 , 0.9940226 , 0.99399555, 0.9939947 ,\n",
      "       0.99399537, 1.0059993 , 0.9940083 , 0.9939953 , 0.9939947 ,\n",
      "       1.0060048 , 1.0060027 , 0.9939951 , 1.0059851 , 0.9939956 ,\n",
      "       0.99399495, 0.99399495, 0.99399495, 1.0059865 , 0.9939974 ,\n",
      "       1.0060042 , 1.0060033 , 0.9940085 , 0.9939966 , 1.0060048 ,\n",
      "       1.0060021 , 1.0060027 , 0.9939951 , 0.99400365, 1.0060048 ,\n",
      "       1.0060042 , 1.0060031 , 1.0060045 , 0.99399716, 1.0060048 ,\n",
      "       0.994139  , 1.0060048 , 0.99399555, 0.9939957 , 1.0060048 ,\n",
      "       0.9939957 , 0.99399763, 0.9939961 , 0.99400336, 0.99399763,\n",
      "       1.0059956 , 0.9939951 , 1.0060045 , 0.99399555, 0.9939952 ,\n",
      "       1.0060048 , 1.0060045 , 1.0060029 , 0.9939951 , 1.0060029 ,\n",
      "       1.0059938 , 0.9939951 , 0.99399644, 1.006004  , 1.0060048 ,\n",
      "       1.0060037 , 1.0059894 , 0.9939957 , 1.0060045 , 1.0060028 ,\n",
      "       1.0060045 , 0.99400264, 0.9939951 , 0.9939971 , 0.9939947 ,\n",
      "       1.0060034 , 0.9939956 , 0.9939946 , 1.0060049 , 0.9939949 ,\n",
      "       1.0060047 , 1.0060045 , 0.99413013, 0.99399495, 1.0060045 ,\n",
      "       0.9939965 , 1.0060029 , 0.9939951 , 1.0059892 , 0.9940024 ,\n",
      "       1.0060045 , 1.0060024 , 1.0060014 , 0.9939961 , 1.0060024 ,\n",
      "       0.99399495, 1.0060042 , 1.0060042 , 0.9939969 , 0.9939947 ,\n",
      "       0.9939949 , 1.0060042 , 0.9940028 , 1.0060033 , 0.9939952 ,\n",
      "       1.0060028 , 1.0060046 , 0.99400073, 1.0060023 , 0.993995  ,\n",
      "       1.0060029 , 0.9939952 , 1.0059999 , 1.0060033 , 0.9939989 ,\n",
      "       1.0060014 , 0.99399585, 0.9939951 , 0.9939994 , 1.005957  ,\n",
      "       1.0060042 , 1.0060045 , 1.0060028 , 0.9939949 , 0.9939985 ,\n",
      "       1.0060048 , 1.0058985 , 1.0060048 , 1.0060043 , 0.99399495,\n",
      "       1.0060045 , 0.9939961 , 1.0059918 , 1.0060043 , 1.0060037 ,\n",
      "       0.99400157, 1.0059923 , 1.0060047 , 0.99399495, 0.99399525,\n",
      "       0.9939965 , 0.9939949 , 0.9939993 , 0.9939996 , 1.0060042 ,\n",
      "       1.0059906 , 1.0059993 , 0.99402803, 1.0060045 , 0.99399555,\n",
      "       0.9939978 , 0.9939961 , 1.0059993 , 0.99399585, 1.0060042 ,\n",
      "       1.0060048 , 0.9939951 , 0.99399483, 0.99399483, 1.0059967 ,\n",
      "       0.9939965 , 1.0060025 , 0.9939949 , 0.99400187, 1.0059929 ,\n",
      "       1.0060042 , 1.006001  , 0.99400306, 1.0060045 , 0.9939951 ,\n",
      "       1.0059782 , 1.006003  , 0.9939947 , 1.0060029 , 0.9939957 ,\n",
      "       1.0059544 , 0.993998  , 0.9939977 , 1.0060027 , 0.99399745,\n",
      "       0.993997  , 0.99399567, 0.99402237, 1.0060045 , 0.99399495,\n",
      "       1.0060021 , 0.9939974 , 1.0060022 , 1.0060016 , 0.9939949 ,\n",
      "       1.0060034 , 0.99399734, 1.0060045 , 0.9939961 , 0.9942477 ,\n",
      "       0.9939949 , 0.99399555, 0.99399817, 0.99399686, 1.0060048 ,\n",
      "       0.9939949 , 1.0060048 , 0.99399567, 1.0060037 , 0.99399465,\n",
      "       1.0059898 , 1.0060023 , 0.99399513, 0.9939974 , 0.99399483,\n",
      "       0.9939978 , 0.9939946 , 0.993995  , 1.0060048 , 1.0060045 ,\n",
      "       1.006004  , 0.99400187, 1.0060022 , 1.0060045 , 1.0060029 ,\n",
      "       1.0060029 , 0.9939949 , 1.0059775 , 1.0060028 , 1.0060045 ,\n",
      "       0.9939949 , 1.0060034 , 0.9939984 , 1.0060042 , 0.99399585,\n",
      "       1.0059978 , 0.99399996, 0.99399674, 1.0060043 , 0.99399644,\n",
      "       1.0060021 , 0.9939957 , 1.0060037 , 1.0060028 , 1.0059501 ,\n",
      "       1.0059842 , 1.0060048 , 0.9944265 , 1.0060014 , 0.99400204,\n",
      "       0.9939947 , 1.0059974 , 0.99399537, 1.0060014 , 0.99399495,\n",
      "       0.9939965 , 0.99399924, 0.9940023 , 1.00597   , 0.9939961 ,\n",
      "       0.993997  , 0.9939957 , 0.9939962 , 0.9940144 , 1.0060047 ,\n",
      "       0.99399513, 0.99400175, 0.9939957 , 0.99399537, 0.99399954,\n",
      "       0.99399626, 0.99400055, 0.9939975 , 1.0060029 , 0.99400425,\n",
      "       1.0060042 , 1.0060043 , 0.9939949 , 0.99399495, 1.0060028 ,\n",
      "       1.0060029 , 0.9939949 , 0.9939947 , 0.9940182 , 0.9939951 ,\n",
      "       0.9939956 , 0.99399734, 0.9940018 , 0.9939962 , 1.0060046 ,\n",
      "       1.0060045 , 0.9939957 , 0.99399495, 0.99400157, 0.9939947 ,\n",
      "       0.9939947 , 1.0060048 , 1.006001  , 1.0060014 , 1.0060045 ,\n",
      "       0.9939949 , 0.9939951 , 0.9939957 , 1.0060029 , 0.9939952 ,\n",
      "       0.993996  , 0.99399745, 0.99400324, 0.99399585, 1.0060045 ,\n",
      "       1.0060024 , 0.99400574, 1.0060011 , 0.99400616, 1.0060023 ,\n",
      "       1.0060048 , 1.0060048 , 0.9939948 , 0.9940058 , 0.99399495,\n",
      "       1.0060048 , 0.99399626, 1.0060042 , 1.0059999 , 0.99399585,\n",
      "       0.9939949 , 1.0060014 , 0.99399513, 0.9939985 , 1.0060046 ,\n",
      "       1.0060045 , 0.99399585, 1.0060029 , 0.99399644, 1.0060021 ,\n",
      "       0.99399614, 1.006     , 1.0060048 , 1.0060045 , 1.0060031 ,\n",
      "       1.0060045 , 0.9939992 , 1.0060022 , 0.9939994 , 0.99399567,\n",
      "       0.9939958 , 1.0060046 , 1.0060045 , 0.99399513, 1.0058988 ,\n",
      "       0.99399585, 0.99399805, 1.0060045 , 1.0060043 , 0.9939983 ,\n",
      "       0.9940259 , 1.0060045 , 1.005951  , 0.99399555, 0.99399495,\n",
      "       0.99400043, 0.99399495, 1.0059993 , 0.9939951 , 1.0060024 ,\n",
      "       1.0060048 , 1.006004  , 1.0059866 , 0.99399495, 0.99399537,\n",
      "       1.0060043 , 0.99399585, 1.0060029 , 1.0059927 , 1.0060012 ,\n",
      "       0.99399585, 1.0060042 , 0.9939949 , 0.99400395, 0.99400496,\n",
      "       0.9940051 , 0.9940081 , 1.0060048 , 0.9939956 , 1.0060049 ,\n",
      "       1.0060042 , 1.0060046 , 1.0060014 , 1.0060042 , 0.993999  ,\n",
      "       1.0060049 , 1.0060033 , 1.0060045 , 0.99399495, 1.0060046 ,\n",
      "       0.9940125 , 1.0060049 , 1.0060048 , 1.0060042 , 1.0060048 ,\n",
      "       0.9939951 , 0.9939951 , 1.0060037 , 0.99399495, 0.9939949 ,\n",
      "       0.99401194, 1.0059923 , 0.99399585, 0.9939947 , 1.0060034 ,\n",
      "       1.0060022 , 1.0060034 , 0.99399585, 0.99399495, 0.9939957 ,\n",
      "       0.99399483, 0.9939965 , 1.0060021 , 1.0059975 , 1.0060045 ,\n",
      "       0.9940018 , 1.0060047 , 1.0060042 , 1.0060045 , 0.994009  ,\n",
      "       0.9939965 , 1.0059876 , 0.9939946 , 1.0060029 , 0.9939946 ,\n",
      "       0.99399585, 1.006002  , 0.99399483, 0.99399555, 0.9939951 ,\n",
      "       1.0060029 , 0.9940047 , 1.0060045 , 0.99399745, 1.0060048 ,\n",
      "       0.99399817, 0.9939951 , 1.0060045 , 0.9939965 , 0.9939949 ,\n",
      "       0.99399537, 0.99399906, 0.99399745, 0.99399495, 0.9939989 ,\n",
      "       1.0060023 , 1.0059934 , 0.99404997, 0.9939949 , 1.0060049 ,\n",
      "       0.9940098 , 0.9939949 , 1.0060048 , 1.0060048 , 1.0060014 ,\n",
      "       1.0060028 , 1.0060048 , 1.0060048 , 0.9939949 , 0.99399495,\n",
      "       0.9940034 , 1.0060048 , 1.0059761 , 0.9940702 , 0.9939965 ,\n",
      "       1.0059384 , 0.9939949 , 1.0060016 , 1.0060048 , 0.99399495,\n",
      "       0.9940062 , 0.9940001 , 1.0059979 , 1.0060046 , 1.0060042 ,\n",
      "       1.0060024 , 0.99399483, 0.9939956 , 1.0060043 , 1.0060029 ,\n",
      "       1.0059906 , 0.9939961 ], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/encoder/encoder_sublayer_1/layer_normalization_3/beta:0' shape=(512,) dtype=float32, numpy=\n",
      "array([-0.00600524, -0.00595517,  0.00599941,  0.00600531,  0.00600486,\n",
      "        0.00600429,  0.00600534, -0.00600519, -0.00600434, -0.00600477,\n",
      "       -0.00600484,  0.00600476,  0.00600489,  0.00600523, -0.00600528,\n",
      "       -0.00600354,  0.00600458,  0.00600172, -0.00600366,  0.00600462,\n",
      "       -0.00600446, -0.00600453,  0.00600508, -0.00599848,  0.00600357,\n",
      "       -0.00600104, -0.00600536,  0.00600495,  0.00600533, -0.00600521,\n",
      "       -0.006005  ,  0.00600528, -0.00600519, -0.00599021, -0.00596452,\n",
      "       -0.00600457, -0.0060053 , -0.00597344,  0.00600515,  0.00600532,\n",
      "        0.00600518, -0.00600533, -0.00600534,  0.00600531, -0.00600532,\n",
      "       -0.00600529,  0.00600485,  0.00600507, -0.00600222, -0.00600442,\n",
      "        0.00600518,  0.00600515, -0.00600495, -0.00600501, -0.0060054 ,\n",
      "       -0.00600484, -0.00600507,  0.00600096,  0.00600418, -0.0060052 ,\n",
      "        0.00600501, -0.00600452,  0.00600536, -0.006004  ,  0.00600526,\n",
      "       -0.00600493,  0.00600434,  0.00600534, -0.00600529,  0.00600537,\n",
      "        0.00600508, -0.00600532,  0.00600404, -0.00600505,  0.00600526,\n",
      "        0.00600398, -0.00600456,  0.00600303, -0.00600516,  0.00600475,\n",
      "       -0.00600504,  0.00600513,  0.00600521,  0.00600229,  0.00600514,\n",
      "        0.00600533,  0.00600534,  0.00600517,  0.0060053 ,  0.00600509,\n",
      "       -0.00600104,  0.00600489,  0.00600539,  0.00600322,  0.0060053 ,\n",
      "        0.00600523, -0.00600454, -0.00600415,  0.00600507,  0.00600407,\n",
      "       -0.0060053 ,  0.006005  , -0.00600523, -0.00600512,  0.00600541,\n",
      "        0.00600516, -0.0060047 ,  0.0060053 , -0.00600539, -0.00600525,\n",
      "        0.00600507,  0.00600501,  0.00600103,  0.00600504,  0.00600518,\n",
      "        0.00600348, -0.00600462,  0.00600508, -0.00600401,  0.00599433,\n",
      "        0.00600521, -0.00600401, -0.00600403,  0.00600522, -0.00600505,\n",
      "        0.00600536,  0.00600523, -0.00600492,  0.00600389, -0.00600538,\n",
      "        0.0060053 ,  0.00600493,  0.00600291,  0.00600468,  0.00600494,\n",
      "       -0.00600538,  0.00600522, -0.00600432,  0.00600478,  0.00600525,\n",
      "       -0.0060026 , -0.00600523, -0.00600365,  0.00600533, -0.00600486,\n",
      "       -0.00600303,  0.00600483,  0.00600528,  0.00600484, -0.00600296,\n",
      "       -0.00600511,  0.0060053 ,  0.00600325,  0.00600521, -0.00600456,\n",
      "       -0.00600529, -0.00600525, -0.00600524, -0.0060051 ,  0.00600529,\n",
      "       -0.00600525, -0.00600535, -0.00600525, -0.00600453, -0.00600501,\n",
      "       -0.0059956 , -0.00600404, -0.00600531, -0.00600523, -0.00600533,\n",
      "       -0.00600459, -0.00600518, -0.00600326, -0.00600528, -0.00600535,\n",
      "       -0.0060032 ,  0.00600466, -0.00600484,  0.00600527,  0.00600503,\n",
      "       -0.00600513, -0.00600536, -0.00599738,  0.00600533, -0.00600471,\n",
      "       -0.00600536, -0.0060053 , -0.00600536,  0.00600518,  0.00600048,\n",
      "        0.0060046 , -0.00600536,  0.00600533, -0.00600457,  0.00600226,\n",
      "        0.00600466, -0.00600306,  0.00600464,  0.00600521,  0.0060051 ,\n",
      "        0.00600425, -0.00600404,  0.00600541, -0.00600365,  0.00600517,\n",
      "       -0.00597772, -0.00600444,  0.00600321, -0.00598881, -0.00600514,\n",
      "        0.00600209, -0.0060053 ,  0.00600419, -0.00600539,  0.00600533,\n",
      "       -0.00600364,  0.00600324, -0.00600375, -0.00600426,  0.0060053 ,\n",
      "        0.00600531,  0.00600369, -0.00600518,  0.00600511,  0.00594656,\n",
      "        0.00600536,  0.00600466,  0.00600478,  0.00600493, -0.0060051 ,\n",
      "        0.00600533, -0.00600527,  0.0060046 , -0.00600528,  0.00600533,\n",
      "        0.00600206, -0.00600146, -0.00600507,  0.00600281,  0.00600534,\n",
      "       -0.00600513,  0.00600542, -0.00600536, -0.00600538, -0.00600495,\n",
      "       -0.00600301, -0.00600399,  0.00600489,  0.00600503,  0.00600509,\n",
      "        0.00600468,  0.0060054 , -0.00600494, -0.0060053 ,  0.00600532,\n",
      "       -0.00600513,  0.00600486, -0.006005  , -0.00600227,  0.00600513,\n",
      "        0.00600521,  0.00600437,  0.00600483, -0.00600517,  0.00600526,\n",
      "        0.00600398,  0.00600479,  0.00600474, -0.00600487, -0.00599717,\n",
      "       -0.00600071, -0.00600518, -0.00600509,  0.00600501,  0.0060047 ,\n",
      "        0.00600528,  0.00599904, -0.00600527,  0.00600292,  0.0060051 ,\n",
      "        0.00600467,  0.00600222,  0.00600497,  0.00598459,  0.00600519,\n",
      "        0.00600429,  0.00600511,  0.00600534,  0.00600128, -0.00600531,\n",
      "        0.00600541,  0.00597029,  0.00600431, -0.00600468,  0.00600516,\n",
      "       -0.00600516,  0.0060032 ,  0.00600357, -0.00600534, -0.00599955,\n",
      "       -0.00600524, -0.00600484,  0.00600525,  0.00600514, -0.00600446,\n",
      "        0.00600273,  0.00600533,  0.00600537,  0.0060029 , -0.00600527,\n",
      "        0.00600524, -0.00600465, -0.00600499, -0.00600444, -0.00600526,\n",
      "        0.00600534,  0.00600517,  0.00600528, -0.00600519,  0.00600535,\n",
      "        0.00600526, -0.00600529,  0.00600526, -0.00600384,  0.00600527,\n",
      "       -0.0060054 ,  0.00600521,  0.00600518, -0.00600411,  0.00600526,\n",
      "        0.00599872,  0.00600458,  0.00600536,  0.00600522, -0.00600504,\n",
      "       -0.00600521,  0.00600498, -0.00600507, -0.00600498, -0.0060053 ,\n",
      "       -0.00600504, -0.00600514,  0.00600508, -0.00600203,  0.00600519,\n",
      "       -0.00600528,  0.00600497, -0.0060046 , -0.00600525,  0.00600489,\n",
      "        0.00600529, -0.00600523,  0.00600523,  0.00600534, -0.00600524,\n",
      "       -0.00600542,  0.00600513,  0.00600489,  0.00600481,  0.0060006 ,\n",
      "        0.00600517,  0.00600469,  0.00600533, -0.00600509,  0.00600445,\n",
      "        0.00600525, -0.00600476, -0.00600535, -0.00600489, -0.00600529,\n",
      "       -0.00600518,  0.00600503,  0.00600532, -0.00600475,  0.00599497,\n",
      "        0.00600491, -0.00600292,  0.00600525,  0.00600517,  0.00600521,\n",
      "        0.0060042 ,  0.00600464,  0.00599356, -0.00600518, -0.00600538,\n",
      "       -0.00599794, -0.00600535,  0.0060031 , -0.00600489, -0.0060054 ,\n",
      "        0.00600529,  0.00600467,  0.00600491, -0.00600507, -0.00600519,\n",
      "        0.0060053 , -0.00600522,  0.00600444,  0.00600522,  0.00600523,\n",
      "       -0.00600525,  0.00600454, -0.00600503,  0.00600475,  0.0060023 ,\n",
      "        0.00600406,  0.00600029,  0.0060053 , -0.00600464,  0.00600533,\n",
      "        0.00600518,  0.00600521,  0.00600472,  0.00600521,  0.00600497,\n",
      "        0.00600539, -0.00600529,  0.00600465, -0.0060052 ,  0.00600541,\n",
      "        0.00600506, -0.00600543,  0.00600524,  0.0060051 ,  0.00600508,\n",
      "       -0.00600517, -0.00600523,  0.00600519, -0.00600518, -0.00600529,\n",
      "       -0.00600327,  0.00600417, -0.00600475, -0.00600528,  0.00600465,\n",
      "        0.0060051 ,  0.00600461, -0.00600498, -0.00600523,  0.00600522,\n",
      "       -0.00600523, -0.00600444,  0.00600385,  0.00600456,  0.00600381,\n",
      "       -0.00599828,  0.00600514,  0.00600493,  0.00600498, -0.00600491,\n",
      "       -0.00600412,  0.00599073, -0.00600539,  0.00600501, -0.00600538,\n",
      "       -0.00600452,  0.00600483, -0.00600537, -0.00600498, -0.00600493,\n",
      "        0.00600386,  0.00600525,  0.00600474,  0.00600512,  0.00600536,\n",
      "       -0.00600532, -0.00600499,  0.00600469, -0.00600522, -0.00600534,\n",
      "       -0.00600492,  0.00600488, -0.00600528, -0.00600525, -0.00600461,\n",
      "       -0.0060054 ,  0.00600233,  0.00599825, -0.00600535,  0.00600526,\n",
      "       -0.00600233, -0.00600506,  0.00600534,  0.00600529,  0.00600346,\n",
      "        0.00600477,  0.00600527,  0.0060054 , -0.00600512, -0.00600522,\n",
      "       -0.00600219,  0.00600519,  0.00600513, -0.00599713,  0.00600527,\n",
      "       -0.00600132, -0.00600525,  0.00596926,  0.00600518, -0.00600497,\n",
      "        0.00600481,  0.00600104, -0.00597852,  0.00600531,  0.00600451,\n",
      "       -0.00600159, -0.00600531,  0.00600502,  0.00600473,  0.00600455,\n",
      "        0.00600225,  0.00600489], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/dense_4/kernel:0' shape=(512, 3) dtype=float32, numpy=\n",
      "array([[ 0.07129701, -0.06378295, -0.10848942],\n",
      "       [ 0.08077457,  0.04998767, -0.03326086],\n",
      "       [-0.01473712, -0.00817442,  0.02761855],\n",
      "       ...,\n",
      "       [-0.00805358,  0.00653062, -0.08051267],\n",
      "       [ 0.06646865,  0.07965513,  0.04570843],\n",
      "       [-0.0894727 , -0.02583816,  0.01497777]], dtype=float32)>\n",
      "<tf.Variable 'space_segmentation_transformer/dense_4/bias:0' shape=(3,) dtype=float32, numpy=array([-0.00600551,  0.00600552, -0.00600547], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "[print(x) for x in model.trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c2409f7cd36a60864259fe7c86cc6f7edd5e2a0604f36f600c4aba8b227f5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
